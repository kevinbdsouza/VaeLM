{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'mr.', '>', 'is', 'chairman', 'of', '>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "a\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "[\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"z\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "29099\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(arg):\n",
    "    return tf.convert_to_tensor(arg,dtype=tf.int32)\n",
    "\n",
    "def embed_producer(sentences):\n",
    "    max_char_len = 494\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    eow_loc_all = np.zeros((len(sentences),max_char_len))\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        eow_loc = np.zeros(max_char_len)\n",
    "        prev = 0\n",
    "        count = 0 \n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    count+=1\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            elif (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                count +=1\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1 \n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        eow_loc_all[i,:] = eow_loc\n",
    "        \n",
    "        \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all,eow_loc_all \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data,word_loc_all,eow_loc_all = embed_producer(sentences)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'downgraded', 'by', 'moody', \"'s\", 'were', 'houston', 'lighting', \"'s\", '>', 'bonds', 'and', 'secured', '>', 'bonds', 'to', 'single-a-3', 'from', 'single-a-2', 'unsecured', '>', 'bonds', 'to', '>', 'from', 'single-a-3', 'preferred', 'stock', 'to', 'single-a-3', 'from', 'single-a-2', 'a', 'shelf', 'registration', 'for', 'preferred', 'stock', 'to', 'a', 'preliminary', 'rating', 'of', 'single-a-3', 'from', 'a', 'preliminary', 'rating', 'of', 'single-a-2', 'two', 'shelf', '>', 'for', 'collateralized', 'debt', 'securities', 'to', 'a', 'preliminary', 'rating', 'of', 'single-a-3', 'from', 'a', 'preliminary', 'rating', 'of', 'single-a-2', 'and', 'the', 'unit', \"'s\", 'rating', 'for', 'commercial', 'paper', 'to', '>', 'from', '>', '<EOS>']\n",
      "[   1.   11.   13.   18.   20.   24.   31.   39.   41.   42.   47.   50.\n",
      "   57.   58.   63.   65.   75.   79.   89.   98.   99.  104.  106.  107.\n",
      "  111.  121.  130.  135.  137.  147.  151.  161.  162.  167.  179.  182.\n",
      "  191.  196.  198.  199.  210.  216.  218.  228.  232.  233.  244.  250.\n",
      "  252.  262.  265.  270.  271.  274.  288.  292.  302.  304.  305.  316.\n",
      "  322.  324.  334.  338.  339.  350.  356.  358.  368.  371.  374.  378.\n",
      "  380.  386.  389.  399.  404.  406.  407.  411.  412.  413.    0.    0.]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  0.  1.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[4607])\n",
    "print(word_loc_all[4607])\n",
    "print(eow_loc_all[4607])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_char_len=494\n",
    "eow_pos = np.zeros((len(sentences),max_char_len))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(max_word_length):\n",
    "        eow_pos[i,int(word_loc_all[i,j])] = 1\n",
    "        \n",
    "print(word_loc_all[29099])\n",
    "print(eow_pos[29099])\n",
    "\n",
    "maxN = 0\n",
    "maxid = 0\n",
    "for i in range(len(word_loc_all)):\n",
    "    if max(word_loc_all[i]) > maxN:\n",
    "        maxN = max(word_loc_all[i])\n",
    "        print(maxN)\n",
    "        maxid = i\n",
    "    \n",
    "print(maxN)\n",
    "print(maxid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "max_char_len = 494\n",
    "batch_size = 52\n",
    "input_size = 61\n",
    "hidden_size=20\n",
    "\n",
    "# our [486, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "inputs = tf.placeholder(tf.float32,[batch_size,max_char_len,input_size])\n",
    "inputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "_inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_char_len,name='char_array')\n",
    "_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='word_array')\n",
    "mean_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='mean_array')\n",
    "sigma_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='sigma_array')\n",
    "word_pos = tf.placeholder(tf.float32,[batch_size,max_char_len])\n",
    "word_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "# create loop_fn for raw_rnn\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "    if cell_output is None:  # time = 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "        sample_loop_state = output_ta\n",
    "        mean_loop_state = mean_ta\n",
    "        sigma_loop_state = sigma_ta\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\n",
    "    else:\n",
    "        word_slice = tf.tile(word_pos[:,time-1],[20])\n",
    "        word_slice = tf.reshape(word_slice,[20,52])\n",
    "        word_slice = tf.transpose(word_slice,perm=[1,0])\n",
    "        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "        \n",
    "        #reparametrization\n",
    "        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*hidden_size)\n",
    "        z_mean = z_concat[:,:20]\n",
    "        z_log_sigma_sq =  z_concat[:,20:40]\n",
    "        eps = tf.random_normal((batch_size,hidden_size),0,1,dtype=tf.float32)\n",
    "        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)),eps))\n",
    "        \n",
    "        z_sample = tf.multiply(z_sample,word_slice)\n",
    "        z_mean = tf.multiply(z_mean,word_slice)\n",
    "        z_log_sigma_sq = tf.multiply(z_log_sigma_sq,word_slice)\n",
    "        \n",
    "        next_cell_state = z_sample\n",
    "        sample_loop_state = loop_state[0].write(time - 1, next_cell_state)\n",
    "        mean_loop_state = loop_state[1].write(time - 1, z_mean)\n",
    "        sigma_loop_state = loop_state[2].write(time - 1, z_log_sigma_sq)\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "        \n",
    "        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "    elements_finished = (time >= max_char_len-1)\n",
    "    next_input = _inputs_ta.read(time)\n",
    "\n",
    "    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "outputs_ta, final_state, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "word_state_out = word_state[0].stack()\n",
    "mean_state_out = word_state[1].stack()\n",
    "sigma_state_out = word_state[2].stack()\n",
    "outputs = outputs_ta.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 494\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "        \n",
    "        for bt in range(1):\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_loc_all[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs, final_state, word_state_out,\n",
    "                                                                   mean_state_out,sigma_state_out],\n",
    "                                                       feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.13253187  0.          0.\n",
      "  0.          0.17314994  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.03053938  0.\n",
      "  0.14000218  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.00622426  0.\n",
      "  0.12139752  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.10800748  0.\n",
      "  0.39081505  0.          0.15581407  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.03140066  0.          0.1720185\n",
      "  0.          0.2231635   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.20605411  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.10539071  0.          0.23587176  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.         0.         0.         0.         0.0951355  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "[ 0.          0.          0.          0.          0.23719308  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.28043571  0.          0.\n",
      "  0.          0.13979147  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.0458578  0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.07615764  0.          0.05308381  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.        0.        0.        0.        0.118598  0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ]\n",
      "[ 0.          0.          0.          0.          0.18050811  0.          0.\n",
      "  0.          0.01390371  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.19290465  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.16620295  0.\n",
      "  0.00801427  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.25603911  0.\n",
      "  0.00755719  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.00078096  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.07006556  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "#print(word_state.shape)\n",
    "#print(mean_state.shape)\n",
    "#print(sigma_state.shape)\n",
    "#word_state = word_state[1]\n",
    "#print(word_state.shape)\n",
    "for i in range(20):\n",
    "    #print(word_state[6,:,i])\n",
    "    #print(mean_state[2,:,i])\n",
    "    #print(sigma_state[3,:,i])\n",
    "    \n",
    "#print(sigma_state[1][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
