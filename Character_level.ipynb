{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'mr.', '>', 'is', 'chairman', 'of', '>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "a\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "[\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"z\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "29099\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "\n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(arg):\n",
    "    return tf.convert_to_tensor(arg,dtype=tf.int32)\n",
    "\n",
    "def embed_producer(sentences):\n",
    "    max_char_len = 486\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        prev = 0\n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        \n",
    "        \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,word_loc_all = embed_producer(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n",
      "[   1.    6.   15.   17.   20.   27.   29.   35.   43.   48.   49.   56.\n",
      "   58.   67.   72.   78.   87.   90.   92.   97.   99.  100.  101.  103.\n",
      "  104.  105.  106.  111.  113.  116.  122.  127.  132.  139.  141.  145.\n",
      "  151.  162.  163.  164.  166.  167.  168.  170.  173.  180.  187.  188.\n",
      "  198.  202.  208.  215.  217.  220.  222.  225.  226.  227.  229.  230.\n",
      "  231.  236.  238.  245.  250.  260.  263.  264.  265.  267.  268.  269.\n",
      "  274.  275.  280.  288.  295.  297.  300.  307.  316.  317.  323.  324.]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])\n",
    "print(word_loc_all[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   7.   8.   9.  14.  17.  21.  25.  28.  33.  35.  36.  48.  56.  60.\n",
      "  61.  62.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[ 1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "max_char_len=486\n",
    "eow_pos = np.zeros((len(sentences),max_char_len))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(max_word_length):\n",
    "        eow_pos[i,int(word_loc_all[i,j])] = 1\n",
    "        \n",
    "print(word_loc_all[29099])\n",
    "print(eow_pos[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413.0\n",
      "4607\n"
     ]
    }
   ],
   "source": [
    "maxN = 0\n",
    "maxid = 0\n",
    "for i in range(len(word_loc_all)):\n",
    "    if max(word_loc_all[i]) > maxN:\n",
    "        maxN = max(word_loc_all[i])\n",
    "        maxid = i\n",
    "    \n",
    "\n",
    "print(maxN)\n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#print(sentences[4607])\n",
    "print(data[4607][432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnetwork_architecture = dict(n_hidden_recog_1 = 500,\\n     n_hidden_recog_2 = 500,\\n     n_input = 20,\\n     n_z = 20)\\n     \\ndef xavier_init(fan_in,fan_out,constant=1):\\n    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\\n    high = constant*np.sqrt(6.0/(fan_in+fan_out))\\n    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\\n    \\n    \\nclass VariationalAutoencoder(object):\\n    \\n    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\\n        self.network_architecture=network_architecture\\n        self.transfer_fct=transfer_fct        \\n        self._create_network()\\n        \\n    def _create_network(self):\\n        \\n        #initialize weights and biases \\n        network_weights = self._initialize_weights(**self.network_architecture)\\n        \\n    \\n    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\\n        all_weights = dict()\\n        all_weights[\"weights_recog\"] = {\\n            \\'h1\\': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\\n            \\'h2\\': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\\n            \\'out_mean\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\\n            \\'out_log_sigma\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \\n\\n        all_weights[\"biases_recog\"]={\\n            \\'b1\\': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\\n            \\'b2\\': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\\n            \\'out_mean\\': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\\n            \\'out_log_sigma\\': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\\n\\n        return all_weights\\n        \\n    def _recognition_network(self,weights,biases):\\n\\n        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights[\\'h1\\']),biases[\\'b1\\']))\\n        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights[\\'h2\\']),biases[\\'b2\\']))\\n\\n        z_mean = tf.add(tf.matmul(layer_2,weights[\\'out_mean\\']),biases[\\'out_mean\\'])\\n        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights[\\'out_log_sigma\\']),biases[\\'out_log_sigma\\'])\\n\\n        return (z_mean,z_log_sigma_sq)\\n        \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "network_architecture = dict(n_hidden_recog_1 = 500,\n",
    "     n_hidden_recog_2 = 500,\n",
    "     n_input = 20,\n",
    "     n_z = 20)\n",
    "     \n",
    "def xavier_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "class VariationalAutoencoder(object):\n",
    "    \n",
    "    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\n",
    "        self.network_architecture=network_architecture\n",
    "        self.transfer_fct=transfer_fct        \n",
    "        self._create_network()\n",
    "        \n",
    "    def _create_network(self):\n",
    "        \n",
    "        #initialize weights and biases \n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "    \n",
    "    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights[\"weights_recog\"] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\n",
    "            'out_mean' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\n",
    "            'out_log_sigma' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \n",
    "\n",
    "        all_weights[\"biases_recog\"]={\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    def _recognition_network(self,weights,biases):\n",
    "\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights['h1']),biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights['h2']),biases['b2']))\n",
    "\n",
    "        z_mean = tf.add(tf.matmul(layer_2,weights['out_mean']),biases['out_mean'])\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights['out_log_sigma']),biases['out_log_sigma'])\n",
    "\n",
    "        return (z_mean,z_log_sigma_sq)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 52\n",
    "input_size = 61\n",
    "hidden_size=20\n",
    "\n",
    "# our [486, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "inputs = tf.placeholder(tf.float32,[batch_size,max_char_len,input_size])\n",
    "inputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "_inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_char_len,name='char_array')\n",
    "_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='word_array')\n",
    "mean_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='mean_array')\n",
    "sigma_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='sigma_array')\n",
    "word_pos = tf.placeholder(tf.float32,[batch_size,max_char_len])\n",
    "word_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "# create loop_fn for raw_rnn\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "    if cell_output is None:  # time = 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "        sample_loop_state = output_ta\n",
    "        mean_loop_state = mean_ta\n",
    "        sigma_loop_state = sigma_ta\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\n",
    "    else:\n",
    "        word_slice = tf.tile(word_pos[:,time],[20])\n",
    "        word_slice = tf.reshape(word_slice,[52,20])\n",
    "        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "        \n",
    "        #reparametrization\n",
    "        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*hidden_size)\n",
    "        z_mean = z_concat[:,:20]\n",
    "        sigma_sq =  z_concat[:,20:40]\n",
    "        eps = tf.random_normal((batch_size,hidden_size),0,1,dtype=tf.float32)\n",
    "        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(sigma_sq),eps))\n",
    "\n",
    "        next_cell_state = z_sample\n",
    "        sample_loop_state = loop_state[0].write(time - 1, next_cell_state)\n",
    "        mean_loop_state = loop_state[1].write(time - 1, z_mean)\n",
    "        sigma_loop_state = loop_state[2].write(time - 1, log_sigma_sq)\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "        \n",
    "        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "    elements_finished = (time >= max_char_len-1)\n",
    "    next_input = _inputs_ta.read(time)\n",
    "\n",
    "    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "outputs_ta, final_state, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "word_state_out = word_state[0].stack()\n",
    "mean_state_out = word_state[1].stack()\n",
    "sigma_state_out = word_state[2].stack()\n",
    "outputs = outputs_ta.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[[  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        ..., \n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02]],\n\n       [[ -2.25064345e-02,  -1.33398990e-03,  -3.07901818e-02, ...,\n          -7.72601590e-02,  -1.75049920e-02,  -3.33631411e-02],\n        [  2.29661111e-02,   3.63297649e-02,   5.05073108e-02, ...,\n           5.06928824e-02,  -4.04723175e-03,  -1.36339320e-02],\n        [  8.25764704e-03,  -4.24458347e-02,  -1.48991682e-02, ...,\n          -6.09258600e-02,  -5.97198904e-02,   3.59318927e-02],\n        ..., \n        [  1.78315770e-02,  -3.40464897e-02,  -5.11401985e-03, ...,\n          -5.03539890e-02,   3.87301706e-02,   3.53762023e-02],\n        [  1.78315770e-02,  -3.40464897e-02,  -5.11401985e-03, ...,\n          -1.60380423e-01,   6.30526692e-02,   3.92417163e-02],\n        [  5.67458495e-02,   2.09928099e-02,  -3.86431068e-02, ...,\n           5.60588390e-02,   3.24297473e-02,   5.22779487e-02]],\n\n       [[ -5.26816286e-02,   2.30218098e-02,  -5.86958835e-03, ...,\n          -4.99221534e-02,  -2.20677089e-02,  -5.32777868e-02],\n        [ -2.34158877e-02,   5.30888550e-02,  -1.51486173e-02, ...,\n           7.89748654e-02,   1.19357621e-02,  -1.35618486e-02],\n        [  2.99027395e-02,  -4.96860556e-02,  -4.09195945e-02, ...,\n          -7.99026340e-02,   3.46820876e-02,   1.13099538e-01],\n        ..., \n        [  2.97895931e-02,   5.47243431e-02,   5.48916217e-03, ...,\n           1.36576323e-02,   5.56362867e-02,   3.02496180e-02],\n        [  4.78815734e-02,   2.92146336e-02,   1.55120967e-02, ...,\n          -6.57284260e-02,   8.53244886e-02,   1.80498436e-02],\n        [  7.74942758e-03,   4.14672606e-02,  -5.28343357e-02, ...,\n           4.13932912e-02,   6.39948323e-02,   8.93490165e-02]],\n\n       ..., \n       [[ -8.77842113e-21,  -1.93106425e-21,   1.84830802e-21, ...,\n          -4.46204648e-21,  -2.16611804e-21,  -2.00932612e-21],\n        [ -9.51515488e-20,  -2.09312997e-20,   2.00342870e-20, ...,\n          -4.83652666e-20,  -2.34791065e-20,  -2.17796058e-20],\n        [  3.38536671e-21,   7.44708121e-22,  -7.12793459e-22, ...,\n           1.72077216e-21,   8.35355736e-22,   7.74889510e-22],\n        ..., \n        [ -2.49235838e-23,  -5.48265389e-24,   5.24769404e-24, ...,\n          -1.26685855e-23,  -6.15001642e-24,  -5.70485313e-24],\n        [ -4.27442229e-21,  -9.40280901e-22,   8.99985128e-22, ...,\n          -2.17267672e-21,  -1.05473424e-21,  -9.78388562e-22],\n        [ -1.33695982e-22,  -2.94102428e-23,   2.81498640e-23, ...,\n          -6.79572810e-23,  -3.29901283e-23,  -3.06021896e-23]],\n\n       [[ -7.65407911e-21,  -1.68373310e-21,   1.61157634e-21, ...,\n          -3.89054627e-21,  -1.88868114e-21,  -1.75197116e-21],\n        [ -8.29645223e-20,  -1.82504138e-20,   1.74682919e-20, ...,\n          -4.21706337e-20,  -2.04718962e-20,  -1.89900684e-20],\n        [  2.95176837e-21,   6.49325775e-22,  -6.21498750e-22, ...,\n           1.50037527e-21,   7.28363226e-22,   6.75641518e-22],\n        ..., \n        [ -2.17313667e-23,  -4.78043437e-24,   4.57556798e-24, ...,\n          -1.10459909e-23,  -5.36232066e-24,  -4.97417467e-24],\n        [ -3.72695353e-21,  -8.19849583e-22,   7.84714854e-22, ...,\n          -1.89439991e-21,  -9.19643693e-22,  -8.53076354e-22],\n        [ -1.16572172e-22,  -2.56433721e-23,   2.45444226e-23, ...,\n          -5.92532990e-23,  -2.87647464e-23,  -2.66826521e-23]],\n\n       [[ -6.67374286e-21,  -1.46808029e-21,   1.40516539e-21, ...,\n          -3.39224427e-21,  -1.64677847e-21,  -1.52757833e-21],\n        [ -7.23384115e-20,  -1.59128967e-20,   1.52309493e-20, ...,\n          -3.67694093e-20,  -1.78498500e-20,  -1.65578149e-20],\n        [  2.57370503e-21,   5.66159994e-22,  -5.41897056e-22, ...,\n           1.30820684e-21,   6.35074277e-22,   5.89105118e-22],\n        ..., \n        [ -1.89480082e-23,  -4.16815525e-24,   3.98952795e-24, ...,\n          -9.63121778e-24,  -4.67551390e-24,  -4.33708113e-24],\n        [ -3.24960448e-21,  -7.14843033e-22,   6.84208421e-22, ...,\n          -1.65176477e-21,  -8.01855476e-22,  -7.43814145e-22],\n        [ -1.01641585e-22,  -2.23589623e-23,   2.14007677e-23, ...,\n          -5.16641257e-23,  -2.50805529e-23,  -2.32651316e-23]]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    276\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 277\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    278\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3411\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3412\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-9599fafa8b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs, final_state, word_state_out,\n\u001b[1;32m     19\u001b[0m                                                                    mean_state_out,sigma_state_out],\n\u001b[0;32m---> 20\u001b[0;31m                                                        feed_dict={inputs:x,word_pos:word_pos_batch})\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    280\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    282\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument array([[[  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        ..., \n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02],\n        [  3.28672715e-02,   9.81817394e-03,  -1.41052762e-02, ...,\n           1.25177139e-02,  -9.83279082e-04,  -3.89110371e-02]],\n\n       [[ -2.25064345e-02,  -1.33398990e-03,  -3.07901818e-02, ...,\n          -7.72601590e-02,  -1.75049920e-02,  -3.33631411e-02],\n        [  2.29661111e-02,   3.63297649e-02,   5.05073108e-02, ...,\n           5.06928824e-02,  -4.04723175e-03,  -1.36339320e-02],\n        [  8.25764704e-03,  -4.24458347e-02,  -1.48991682e-02, ...,\n          -6.09258600e-02,  -5.97198904e-02,   3.59318927e-02],\n        ..., \n        [  1.78315770e-02,  -3.40464897e-02,  -5.11401985e-03, ...,\n          -5.03539890e-02,   3.87301706e-02,   3.53762023e-02],\n        [  1.78315770e-02,  -3.40464897e-02,  -5.11401985e-03, ...,\n          -1.60380423e-01,   6.30526692e-02,   3.92417163e-02],\n        [  5.67458495e-02,   2.09928099e-02,  -3.86431068e-02, ...,\n           5.60588390e-02,   3.24297473e-02,   5.22779487e-02]],\n\n       [[ -5.26816286e-02,   2.30218098e-02,  -5.86958835e-03, ...,\n          -4.99221534e-02,  -2.20677089e-02,  -5.32777868e-02],\n        [ -2.34158877e-02,   5.30888550e-02,  -1.51486173e-02, ...,\n           7.89748654e-02,   1.19357621e-02,  -1.35618486e-02],\n        [  2.99027395e-02,  -4.96860556e-02,  -4.09195945e-02, ...,\n          -7.99026340e-02,   3.46820876e-02,   1.13099538e-01],\n        ..., \n        [  2.97895931e-02,   5.47243431e-02,   5.48916217e-03, ...,\n           1.36576323e-02,   5.56362867e-02,   3.02496180e-02],\n        [  4.78815734e-02,   2.92146336e-02,   1.55120967e-02, ...,\n          -6.57284260e-02,   8.53244886e-02,   1.80498436e-02],\n        [  7.74942758e-03,   4.14672606e-02,  -5.28343357e-02, ...,\n           4.13932912e-02,   6.39948323e-02,   8.93490165e-02]],\n\n       ..., \n       [[ -8.77842113e-21,  -1.93106425e-21,   1.84830802e-21, ...,\n          -4.46204648e-21,  -2.16611804e-21,  -2.00932612e-21],\n        [ -9.51515488e-20,  -2.09312997e-20,   2.00342870e-20, ...,\n          -4.83652666e-20,  -2.34791065e-20,  -2.17796058e-20],\n        [  3.38536671e-21,   7.44708121e-22,  -7.12793459e-22, ...,\n           1.72077216e-21,   8.35355736e-22,   7.74889510e-22],\n        ..., \n        [ -2.49235838e-23,  -5.48265389e-24,   5.24769404e-24, ...,\n          -1.26685855e-23,  -6.15001642e-24,  -5.70485313e-24],\n        [ -4.27442229e-21,  -9.40280901e-22,   8.99985128e-22, ...,\n          -2.17267672e-21,  -1.05473424e-21,  -9.78388562e-22],\n        [ -1.33695982e-22,  -2.94102428e-23,   2.81498640e-23, ...,\n          -6.79572810e-23,  -3.29901283e-23,  -3.06021896e-23]],\n\n       [[ -7.65407911e-21,  -1.68373310e-21,   1.61157634e-21, ...,\n          -3.89054627e-21,  -1.88868114e-21,  -1.75197116e-21],\n        [ -8.29645223e-20,  -1.82504138e-20,   1.74682919e-20, ...,\n          -4.21706337e-20,  -2.04718962e-20,  -1.89900684e-20],\n        [  2.95176837e-21,   6.49325775e-22,  -6.21498750e-22, ...,\n           1.50037527e-21,   7.28363226e-22,   6.75641518e-22],\n        ..., \n        [ -2.17313667e-23,  -4.78043437e-24,   4.57556798e-24, ...,\n          -1.10459909e-23,  -5.36232066e-24,  -4.97417467e-24],\n        [ -3.72695353e-21,  -8.19849583e-22,   7.84714854e-22, ...,\n          -1.89439991e-21,  -9.19643693e-22,  -8.53076354e-22],\n        [ -1.16572172e-22,  -2.56433721e-23,   2.45444226e-23, ...,\n          -5.92532990e-23,  -2.87647464e-23,  -2.66826521e-23]],\n\n       [[ -6.67374286e-21,  -1.46808029e-21,   1.40516539e-21, ...,\n          -3.39224427e-21,  -1.64677847e-21,  -1.52757833e-21],\n        [ -7.23384115e-20,  -1.59128967e-20,   1.52309493e-20, ...,\n          -3.67694093e-20,  -1.78498500e-20,  -1.65578149e-20],\n        [  2.57370503e-21,   5.66159994e-22,  -5.41897056e-22, ...,\n           1.30820684e-21,   6.35074277e-22,   5.89105118e-22],\n        ..., \n        [ -1.89480082e-23,  -4.16815525e-24,   3.98952795e-24, ...,\n          -9.63121778e-24,  -4.67551390e-24,  -4.33708113e-24],\n        [ -3.24960448e-21,  -7.14843033e-22,   6.84208421e-22, ...,\n          -1.65176477e-21,  -8.01855476e-22,  -7.43814145e-22],\n        [ -1.01641585e-22,  -2.23589623e-23,   2.14007677e-23, ...,\n          -5.16641257e-23,  -2.50805529e-23,  -2.32651316e-23]]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 486\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "        \n",
    "        for bt in range(2):\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_pos[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs, final_state, word_state_out,\n",
    "                                                                   mean_state_out,sigma_state_out],\n",
    "                                                       feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486, 52, 20)\n",
      "(486, 52, 20)\n",
      "(486, 52, 20)\n",
      "[ 0.01565862  0.          0.          0.07048745  0.          0.00279995\n",
      "  0.          0.01728558  0.00103465  0.00661615  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.00883843  0.00778908\n",
      "  0.          0.00114315  0.          0.02052551  0.          0.00663459\n",
      "  0.00453121  0.01899582  0.          0.          0.          0.00759071\n",
      "  0.00604148  0.02583527  0.          0.          0.02910638  0.\n",
      "  0.00747149  0.          0.02005519  0.          0.          0.\n",
      "  0.00530694  0.          0.          0.01105913  0.00435434  0.01112524\n",
      "  0.0094572   0.00841358  0.        ]\n",
      "[ 0.          0.          0.00942982  0.05118716  0.          0.          0.\n",
      "  0.          0.02239961  0.01064624  0.          0.0404136   0.\n",
      "  0.02078239  0.00492252  0.00401987  0.          0.0064113   0.          0.\n",
      "  0.          0.          0.          0.          0.01384959  0.009954    0.\n",
      "  0.02953841  0.0082041   0.          0.00550621  0.          0.01061924\n",
      "  0.00563432  0.          0.          0.          0.0147872   0.          0.\n",
      "  0.00912119  0.00555774  0.          0.00384959  0.00539808  0.          0.\n",
      "  0.          0.02031547  0.          0.00101862  0.        ]\n",
      "[ 0.          0.          0.          0.02195597  0.          0.          0.\n",
      "  0.          0.          0.00230948  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.00323734  0.00075174  0.00163521\n",
      "  0.          0.          0.00628485  0.01970985  0.          0.01016722\n",
      "  0.          0.          0.          0.          0.00278033  0.\n",
      "  0.00516145  0.0140537   0.0041883   0.00868348  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.00194383  0.          0.\n",
      "  0.          0.          0.00283225  0.00143389  0.00166145  0.        ]\n",
      "[ 0.          0.00299893  0.01167396  0.          0.00224057  0.\n",
      "  0.00336837  0.00377497  0.00893088  0.0035685   0.00814583  0.\n",
      "  0.00723537  0.02550447  0.00882086  0.00497654  0.          0.\n",
      "  0.00515361  0.00590271  0.00024965  0.00957179  0.          0.          0.\n",
      "  0.          0.          0.          0.01015654  0.00747976  0.\n",
      "  0.00124238  0.          0.          0.          0.          0.01187399\n",
      "  0.          0.01268387  0.00236823  0.00587222  0.00688039  0.          0.\n",
      "  0.          0.01405032  0.00241519  0.          0.          0.          0.\n",
      "  0.0144596 ]\n",
      "[ 0.          0.          0.          0.05981897  0.          0.          0.\n",
      "  0.          0.0107108   0.00866454  0.          0.          0.00943091\n",
      "  0.          0.00129352  0.          0.          0.01389445  0.00266972\n",
      "  0.          0.          0.          0.          0.          0.00500981\n",
      "  0.00204093  0.          0.01643481  0.          0.          0.01193297\n",
      "  0.          0.03264777  0.01256457  0.          0.          0.\n",
      "  0.00954319  0.          0.          0.00943286  0.          0.\n",
      "  0.00834278  0.          0.          0.          0.00298472  0.\n",
      "  0.00526269  0.00857825  0.01811479]\n",
      "[  1.69458799e-02   0.00000000e+00   0.00000000e+00   3.44192348e-02\n",
      "   1.49942725e-03   2.53147865e-03   0.00000000e+00   1.55780213e-02\n",
      "   0.00000000e+00   1.26804067e-02   0.00000000e+00   0.00000000e+00\n",
      "   3.51535412e-03   0.00000000e+00   1.80386305e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.75943631e-03   0.00000000e+00\n",
      "   1.03022368e-03   0.00000000e+00   0.00000000e+00   2.60895230e-02\n",
      "   0.00000000e+00   1.01083461e-02   2.04389170e-02   6.94307499e-04\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.52836929e-03\n",
      "   1.28397038e-02   0.00000000e+00   7.52587104e-03   6.01735571e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.37002645e-02\n",
      "   1.59485601e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   9.96665470e-03   4.11458313e-06\n",
      "   8.46918300e-03   2.47658929e-03   4.27401392e-03   6.20688871e-03]\n",
      "[ 0.          0.          0.          0.00997821  0.          0.          0.\n",
      "  0.          0.00068902  0.0073162   0.          0.00240676  0.00342666\n",
      "  0.          0.          0.          0.00369037  0.00457309  0.00156923\n",
      "  0.0114507   0.          0.          0.00379128  0.03168018  0.00605877\n",
      "  0.00661517  0.          0.          0.          0.          0.00392751\n",
      "  0.          0.          0.00571591  0.00576207  0.00561633  0.\n",
      "  0.01592936  0.          0.00248968  0.          0.          0.\n",
      "  0.00274586  0.          0.          0.          0.          0.0003699\n",
      "  0.00217571  0.00650781  0.00623921]\n",
      "[  3.04057728e-03   0.00000000e+00   0.00000000e+00   5.02807312e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.43707109e-04\n",
      "   6.09999802e-03   5.99853974e-03   0.00000000e+00   0.00000000e+00\n",
      "   2.92445649e-03   1.42448675e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.38175227e-03   0.00000000e+00   0.00000000e+00\n",
      "   3.59570658e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   5.93091594e-03   3.58022330e-03   2.81098136e-03\n",
      "   0.00000000e+00   0.00000000e+00   4.62201145e-03   0.00000000e+00\n",
      "   9.67146829e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.23141622e-03\n",
      "   0.00000000e+00   0.00000000e+00   3.47858120e-04   0.00000000e+00\n",
      "   4.33304394e-03   2.67155655e-03   0.00000000e+00   5.30816242e-03]\n",
      "[ 0.02025147  0.          0.          0.          0.00063248  0.00916711\n",
      "  0.01321897  0.00932674  0.          0.          0.01231432  0.00266816\n",
      "  0.          0.          0.02279163  0.          0.          0.\n",
      "  0.00904618  0.          0.00061681  0.          0.00218731  0.\n",
      "  0.01493366  0.          0.02439037  0.00736796  0.          0.01633277\n",
      "  0.          0.01284347  0.          0.          0.          0.00691444\n",
      "  0.01963742  0.00295895  0.00973292  0.          0.02185672  0.\n",
      "  0.02438173  0.          0.          0.02886629  0.00596715  0.01142341\n",
      "  0.01079715  0.          0.01139663  0.        ]\n",
      "[ 0.01760796  0.          0.          0.02565126  0.          0.00032246\n",
      "  0.00457212  0.01543711  0.01225806  0.          0.00774509  0.          0.\n",
      "  0.0132677   0.          0.          0.          0.0080818   0.00999446\n",
      "  0.          0.0010209   0.          0.          0.          0.00393462\n",
      "  0.          0.0202476   0.          0.          0.          0.00694089\n",
      "  0.00463231  0.01533128  0.          0.          0.          0.\n",
      "  0.00863075  0.00396008  0.0003002   0.          0.          0.\n",
      "  0.00485263  0.          0.          0.0098765   0.00533929  0.00356825\n",
      "  0.0021763   0.00691309  0.        ]\n",
      "[ 0.0218051   0.          0.          0.          0.          0.0136355\n",
      "  0.00422571  0.02016039  0.00126864  0.          0.00816881  0.00384883\n",
      "  0.          0.01198145  0.01748773  0.          0.          0.00165184\n",
      "  0.00471774  0.          0.00133327  0.          0.          0.\n",
      "  0.01057821  0.          0.02523391  0.01391031  0.          0.\n",
      "  0.00141865  0.01296988  0.00063408  0.          0.          0.\n",
      "  0.00489262  0.          0.00554978  0.00454676  0.01848366  0.\n",
      "  0.02458362  0.00099183  0.          0.          0.01289841  0.00686315\n",
      "  0.          0.          0.0064319   0.        ]\n",
      "[ 0.          0.00973138  0.00577899  0.          0.00511824  0.00188414\n",
      "  0.01393844  0.          0.          0.          0.01591249  0.00221533\n",
      "  0.01156293  0.00022253  0.00990192  0.00246354  0.00582182  0.\n",
      "  0.00044523  0.00794293  0.          0.01161626  0.          0.00617619\n",
      "  0.00218412  0.          0.          0.          0.00502781  0.00862842\n",
      "  0.          0.00310434  0.          0.01094099  0.00236597  0.\n",
      "  0.02270979  0.          0.02516335  0.          0.0031589   0.00340601\n",
      "  0.02176801  0.          0.00897579  0.02873802  0.          0.          0.\n",
      "  0.00118156  0.00014879  0.02327159]\n",
      "[ 0.          0.01022222  0.00910281  0.03295254  0.00412178  0.          0.\n",
      "  0.          0.          0.00617024  0.          0.00672004  0.00282038\n",
      "  0.01177453  0.01797282  0.00388047  0.          0.          0.          0.\n",
      "  0.          0.01026116  0.          0.01939744  0.          0.01122397\n",
      "  0.          0.00956854  0.00791959  0.          0.          0.\n",
      "  0.02055092  0.00542134  0.00348967  0.          0.          0.          0.\n",
      "  0.          0.01285285  0.005365    0.00412466  0.          0.00546052\n",
      "  0.          0.          0.          0.00178346  0.          0.\n",
      "  0.00479813]\n",
      "[ 0.          0.          0.          0.          0.          0.00319999\n",
      "  0.00411037  0.          0.0068529   0.          0.0015125   0.00131294\n",
      "  0.00262805  0.          0.          0.          0.          0.00803022\n",
      "  0.          0.01559552  0.          0.          0.00561341  0.          0.\n",
      "  0.          0.          0.0016934   0.          0.00797605  0.0068966\n",
      "  0.00023531  0.          0.0039552   0.          0.          0.01209205\n",
      "  0.          0.00432693  0.00515427  0.          0.          0.02694008\n",
      "  0.00482166  0.00761561  0.01459475  0.          0.00453834  0.          0.\n",
      "  0.          0.00524268]\n",
      "[ 0.          0.          0.01033885  0.          0.          0.01160062\n",
      "  0.00484812  0.00934958  0.02714946  0.01192239  0.          0.          0.\n",
      "  0.02184809  0.00213414  0.00440739  0.02911065  0.01360264  0.\n",
      "  0.01183464  0.00061832  0.          0.00801419  0.0331571   0.00898644\n",
      "  0.01102065  0.          0.0134215   0.00899497  0.02810546  0.01168236\n",
      "  0.0113674   0.          0.          0.00013691  0.01657328  0.          0.\n",
      "  0.          0.01213238  0.0100289   0.0060935   0.00989189  0.00816756\n",
      "  0.          0.01318651  0.00598177  0.          0.01176365  0.\n",
      "  0.00877435  0.        ]\n",
      "[  0.00000000e+00   7.45337643e-03   0.00000000e+00   2.25778595e-02\n",
      "   5.26825525e-03   3.02785682e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   4.60922671e-03   2.26673037e-02   7.91594945e-03\n",
      "   1.17843933e-02   0.00000000e+00   2.44868398e-02   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.15093961e-03   0.00000000e+00\n",
      "   0.00000000e+00   8.77728686e-03   0.00000000e+00   0.00000000e+00\n",
      "   8.49188119e-03   0.00000000e+00   0.00000000e+00   4.59819566e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   5.28087560e-03\n",
      "   7.37297628e-03   1.39786787e-02   4.73291567e-03   0.00000000e+00\n",
      "   6.33528596e-03   0.00000000e+00   2.76494492e-02   0.00000000e+00\n",
      "   1.76412724e-02   0.00000000e+00   1.03219841e-02   0.00000000e+00\n",
      "   1.00467792e-02   0.00000000e+00   0.00000000e+00   4.43262281e-04\n",
      "   0.00000000e+00   1.75283551e-02   3.76589363e-03   2.37842947e-02]\n",
      "[ 0.          0.          0.00587524  0.          0.          0.          0.\n",
      "  0.          0.01395093  0.          0.          0.02685236  0.00644105\n",
      "  0.          0.          0.00250458  0.          0.00375447  0.\n",
      "  0.01057998  0.          0.          0.          0.          0.0154199\n",
      "  0.01096898  0.          0.00335588  0.00511155  0.00961927  0.00322445\n",
      "  0.          0.0056511   0.00774512  0.          0.          0.\n",
      "  0.01126099  0.          0.00261229  0.          0.00346274  0.\n",
      "  0.00225433  0.00293367  0.          0.          0.          0.          0.\n",
      "  0.00578775  0.01181326]\n",
      "[ 0.          0.00374158  0.          0.04129732  0.          0.          0.\n",
      "  0.          0.          0.          0.02039045  0.02142459  0.00215515\n",
      "  0.          0.01205356  0.          0.          0.00154015  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.03287297  0.          0.          0.00132273  0.          0.01303736\n",
      "  0.01413571  0.00433659  0.          0.          0.01384326  0.01958678\n",
      "  0.          0.01349989  0.          0.          0.00092477  0.00850199\n",
      "  0.          0.          0.00081946  0.          0.02023564  0.\n",
      "  0.00483536]\n",
      "[  6.92692166e-03   7.82681443e-03   7.27030868e-03   0.00000000e+00\n",
      "   2.01621489e-03   5.45802945e-03   1.57531388e-02   1.45651465e-02\n",
      "   6.54844847e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.32541289e-04   2.80146506e-02   0.00000000e+00   3.09928576e-03\n",
      "   2.63085286e-03   0.00000000e+00   0.00000000e+00   9.23848152e-03\n",
      "   9.63239116e-04   8.52935668e-03   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   8.78369063e-03   6.83230115e-03   0.00000000e+00\n",
      "   6.32528588e-03   3.17801461e-02   0.00000000e+00   2.90228054e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.33375782e-03   0.00000000e+00   0.00000000e+00   3.66017781e-03\n",
      "   0.00000000e+00   4.28496581e-03   1.85192786e-02   0.00000000e+00\n",
      "   0.00000000e+00   4.26853411e-02   9.31862835e-03   2.38194130e-03\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[ 0.00322499  0.00998267  0.01075324  0.00852975  0.00677958  0.00132353\n",
      "  0.01286946  0.00702173  0.          0.00834749  0.          0.\n",
      "  0.00293911  0.03662169  0.01288149  0.00458404  0.          0.\n",
      "  0.00407505  0.          0.00046437  0.01794332  0.          0.          0.\n",
      "  0.00110095  0.00220244  0.          0.00935549  0.          0.\n",
      "  0.00347973  0.00744019  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.0037761   0.00633773  0.          0.          0.\n",
      "  0.          0.00449243  0.          0.01322298  0.          0.          0.0056183 ]\n"
     ]
    }
   ],
   "source": [
    "print(word_state.shape)\n",
    "print(mean_state.shape)\n",
    "print(sigma_state.shape)\n",
    "#word_state = word_state[1]\n",
    "#print(word_state.shape)\n",
    "for i in range(20):\n",
    "    #print(word_state[2,:,i])\n",
    "    #print(mean_state[2,:,i])\n",
    "    print(sigma_state[2,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
