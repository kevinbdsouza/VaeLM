{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'mr.', '>', 'is', 'chairman', 'of', '>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "a\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "[\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"z\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "29099\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "\n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(arg):\n",
    "    return tf.convert_to_tensor(arg,dtype=tf.int32)\n",
    "\n",
    "def embed_producer(sentences):\n",
    "    max_char_len = 486\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        prev = 0\n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        \n",
    "        \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,word_loc_all = embed_producer(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n",
      "[   1.    6.   15.   17.   20.   27.   29.   35.   43.   48.   49.   56.\n",
      "   58.   67.   72.   78.   87.   90.   92.   97.   99.  100.  101.  103.\n",
      "  104.  105.  106.  111.  113.  116.  122.  127.  132.  139.  141.  145.\n",
      "  151.  162.  163.  164.  166.  167.  168.  170.  173.  180.  187.  188.\n",
      "  198.  202.  208.  215.  217.  220.  222.  225.  226.  227.  229.  230.\n",
      "  231.  236.  238.  245.  250.  260.  263.  264.  265.  267.  268.  269.\n",
      "  274.  275.  280.  288.  295.  297.  300.  307.  316.  317.  323.  324.]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])\n",
    "print(word_loc_all[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   7.   8.   9.  14.  17.  21.  25.  28.  33.  35.  36.  48.  56.  60.\n",
      "  61.  62.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[ 1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "max_char_len=486\n",
    "eow_pos = np.zeros((len(sentences),max_char_len))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(max_word_length):\n",
    "        eow_pos[i,int(word_loc_all[i,j])] = 1\n",
    "        \n",
    "print(word_loc_all[29099])\n",
    "print(eow_pos[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413.0\n",
      "4607\n"
     ]
    }
   ],
   "source": [
    "maxN = 0\n",
    "maxid = 0\n",
    "for i in range(len(word_loc_all)):\n",
    "    if max(word_loc_all[i]) > maxN:\n",
    "        maxN = max(word_loc_all[i])\n",
    "        maxid = i\n",
    "    \n",
    "\n",
    "print(maxN)\n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#print(sentences[4607])\n",
    "print(data[4607][432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnetwork_architecture = dict(n_hidden_recog_1 = 500,\\n     n_hidden_recog_2 = 500,\\n     n_input = 20,\\n     n_z = 20)\\n     \\ndef xavier_init(fan_in,fan_out,constant=1):\\n    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\\n    high = constant*np.sqrt(6.0/(fan_in+fan_out))\\n    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\\n    \\n    \\nclass VariationalAutoencoder(object):\\n    \\n    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\\n        self.network_architecture=network_architecture\\n        self.transfer_fct=transfer_fct        \\n        self._create_network()\\n        \\n    def _create_network(self):\\n        \\n        #initialize weights and biases \\n        network_weights = self._initialize_weights(**self.network_architecture)\\n        \\n    \\n    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\\n        all_weights = dict()\\n        all_weights[\"weights_recog\"] = {\\n            \\'h1\\': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\\n            \\'h2\\': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\\n            \\'out_mean\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\\n            \\'out_log_sigma\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \\n\\n        all_weights[\"biases_recog\"]={\\n            \\'b1\\': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\\n            \\'b2\\': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\\n            \\'out_mean\\': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\\n            \\'out_log_sigma\\': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\\n\\n        return all_weights\\n        \\n    def _recognition_network(self,weights,biases):\\n\\n        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights[\\'h1\\']),biases[\\'b1\\']))\\n        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights[\\'h2\\']),biases[\\'b2\\']))\\n\\n        z_mean = tf.add(tf.matmul(layer_2,weights[\\'out_mean\\']),biases[\\'out_mean\\'])\\n        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights[\\'out_log_sigma\\']),biases[\\'out_log_sigma\\'])\\n\\n        return (z_mean,z_log_sigma_sq)\\n        \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "network_architecture = dict(n_hidden_recog_1 = 500,\n",
    "     n_hidden_recog_2 = 500,\n",
    "     n_input = 20,\n",
    "     n_z = 20)\n",
    "     \n",
    "def xavier_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "class VariationalAutoencoder(object):\n",
    "    \n",
    "    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\n",
    "        self.network_architecture=network_architecture\n",
    "        self.transfer_fct=transfer_fct        \n",
    "        self._create_network()\n",
    "        \n",
    "    def _create_network(self):\n",
    "        \n",
    "        #initialize weights and biases \n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "    \n",
    "    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights[\"weights_recog\"] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\n",
    "            'out_mean' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\n",
    "            'out_log_sigma' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \n",
    "\n",
    "        all_weights[\"biases_recog\"]={\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    def _recognition_network(self,weights,biases):\n",
    "\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights['h1']),biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights['h2']),biases['b2']))\n",
    "\n",
    "        z_mean = tf.add(tf.matmul(layer_2,weights['out_mean']),biases['out_mean'])\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights['out_log_sigma']),biases['out_log_sigma'])\n",
    "\n",
    "        return (z_mean,z_log_sigma_sq)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 52\n",
    "input_size = 61\n",
    "hidden_size=20\n",
    "\n",
    "# our [486, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "inputs = tf.placeholder(tf.float32,[batch_size,max_char_len,input_size])\n",
    "inputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "_inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_char_len,name='char_array')\n",
    "_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='word_array')\n",
    "mean_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='mean_array')\n",
    "sigma_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='sigma_array')\n",
    "word_pos = tf.placeholder(tf.float32,[batch_size,max_char_len])\n",
    "word_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "# create loop_fn for raw_rnn\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "    if cell_output is None:  # time = 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "        sample_loop_state = output_ta\n",
    "        mean_loop_state = mean_ta\n",
    "        sigma_loop_state = sigma_ta\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\n",
    "    else:\n",
    "        word_slice = tf.tile(word_pos[:,time],[20])\n",
    "        word_slice = tf.reshape(word_slice,[20,52])\n",
    "        word_slice = tf.transpose(word_slice,perm=[1,0])\n",
    "        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "        \n",
    "        #reparametrization\n",
    "        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*hidden_size)\n",
    "        z_mean = z_concat[:,:20]\n",
    "        z_log_sigma_sq =  z_concat[:,20:40]\n",
    "        eps = tf.random_normal((batch_size,hidden_size),0,1,dtype=tf.float32)\n",
    "        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)),eps))\n",
    "        \n",
    "        z_sample = tf.multiply(z_sample,word_slice)\n",
    "        #z_mean = tf.multiply(z_mean,word_slice)\n",
    "        #z_log_sigma_sq = tf.multiply(z_log_sigma_sq,word_slice)\n",
    "        \n",
    "        next_cell_state = z_sample\n",
    "        sample_loop_state = loop_state[0].write(time - 1, next_cell_state)\n",
    "        mean_loop_state = loop_state[1].write(time - 1, z_mean)\n",
    "        sigma_loop_state = loop_state[2].write(time - 1, z_log_sigma_sq)\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "        \n",
    "        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "    elements_finished = (time >= max_char_len-1)\n",
    "    next_input = _inputs_ta.read(time)\n",
    "\n",
    "    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "outputs_ta, final_state, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "word_state_out = word_state[0].stack()\n",
    "mean_state_out = word_state[1].stack()\n",
    "sigma_state_out = word_state[2].stack()\n",
    "outputs = outputs_ta.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 486\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "        \n",
    "        for bt in range(1):\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_pos[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs, final_state, word_state_out,\n",
    "                                                                   mean_state_out,sigma_state_out],\n",
    "                                                       feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339  0.02326339\n",
      "  0.02326339  0.02326339  0.02326339  0.02326339]\n",
      "[ 0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425  0.01731425\n",
      "  0.01731425  0.01731425  0.01731425  0.01731425]\n",
      "[ 0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538  0.01644538\n",
      "  0.01644538  0.01644538  0.01644538  0.01644538]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173  0.00098173\n",
      "  0.00098173  0.00098173  0.00098173  0.00098173]\n",
      "[ 0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949  0.01121949\n",
      "  0.01121949  0.01121949  0.01121949  0.01121949]\n",
      "[ 0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702  0.01807702\n",
      "  0.01807702  0.01807702  0.01807702  0.01807702]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897  0.0259897\n",
      "  0.0259897  0.0259897  0.0259897  0.0259897]\n",
      "[ 0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586  0.01904586\n",
      "  0.01904586  0.01904586  0.01904586  0.01904586]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346  0.01278346\n",
      "  0.01278346  0.01278346  0.01278346  0.01278346]\n",
      "[ 0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443  0.01119443\n",
      "  0.01119443  0.01119443  0.01119443  0.01119443]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733  0.01348733\n",
      "  0.01348733  0.01348733  0.01348733  0.01348733]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232  0.01252232\n",
      "  0.01252232  0.01252232  0.01252232  0.01252232]\n"
     ]
    }
   ],
   "source": [
    "#print(word_state.shape)\n",
    "#print(mean_state.shape)\n",
    "#print(sigma_state.shape)\n",
    "#word_state = word_state[1]\n",
    "#print(word_state.shape)\n",
    "for i in range(20):\n",
    "    #print(word_state[0,:,i])\n",
    "    #print(mean_state[2,:,i])\n",
    "    print(sigma_state[0,:,i])\n",
    "    \n",
    "#print(sigma_state[1][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
