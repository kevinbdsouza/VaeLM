{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'mr.', '>', 'is', 'chairman', 'of', '>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "a\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "[\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"z\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "29099\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "\n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(arg):\n",
    "    return tf.convert_to_tensor(arg,dtype=tf.int32)\n",
    "\n",
    "def embed_producer(sentences):\n",
    "    max_char_len = 486\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        prev = 0\n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        \n",
    "        \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,word_loc_all = embed_producer(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n",
      "[   1.    6.   15.   17.   20.   27.   29.   35.   43.   48.   49.   56.\n",
      "   58.   67.   72.   78.   87.   90.   92.   97.   99.  100.  101.  103.\n",
      "  104.  105.  106.  111.  113.  116.  122.  127.  132.  139.  141.  145.\n",
      "  151.  162.  163.  164.  166.  167.  168.  170.  173.  180.  187.  188.\n",
      "  198.  202.  208.  215.  217.  220.  222.  225.  226.  227.  229.  230.\n",
      "  231.  236.  238.  245.  250.  260.  263.  264.  265.  267.  268.  269.\n",
      "  274.  275.  280.  288.  295.  297.  300.  307.  316.  317.  323.  324.]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])\n",
    "print(word_loc_all[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   7.   8.   9.  14.  17.  21.  25.  28.  33.  35.  36.  48.  56.  60.\n",
      "  61.  62.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[ 1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "max_char_len=486\n",
    "eow_pos = np.zeros((len(sentences),max_char_len))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(max_word_length):\n",
    "        eow_pos[i,int(word_loc_all[i,j])] = 1\n",
    "        \n",
    "print(word_loc_all[29099])\n",
    "print(eow_pos[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413.0\n",
      "4607\n"
     ]
    }
   ],
   "source": [
    "maxN = 0\n",
    "maxid = 0\n",
    "for i in range(len(word_loc_all)):\n",
    "    if max(word_loc_all[i]) > maxN:\n",
    "        maxN = max(word_loc_all[i])\n",
    "        maxid = i\n",
    "    \n",
    "\n",
    "print(maxN)\n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#print(sentences[4607])\n",
    "print(data[4607][432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'using', 'estimates', 'of', 'the', 'company', \"'s\", 'future', 'earnings', 'under', 'a', 'variety', 'of', 'scenarios', 'first', 'boston', 'estimated', 'ual', \"'s\", 'value', 'at', '$', 'n', 'to', '$', 'n', 'a', 'share', 'if', 'its', 'future', 'labor', 'costs', 'conform', 'to', 'wall', 'street', 'projections', '$', 'n', 'to', '$', 'n', 'if', 'the', 'company', 'reaches', 'a', 'settlement', 'with', 'pilots', 'similar', 'to', 'one', 'at', 'nwa', '$', 'n', 'to', '$', 'n', 'under', 'an', 'adverse', 'labor', 'settlement', 'and', '$', 'n', 'to', '$', 'n', 'under', 'a', 'pilot', 'contract', 'imposed', 'by', 'the', 'company', 'following', 'a', 'strike', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnetwork_architecture = dict(n_hidden_recog_1 = 500,\\n     n_hidden_recog_2 = 500,\\n     n_input = 20,\\n     n_z = 20)\\n     \\ndef xavier_init(fan_in,fan_out,constant=1):\\n    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\\n    high = constant*np.sqrt(6.0/(fan_in+fan_out))\\n    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\\n    \\n    \\nclass VariationalAutoencoder(object):\\n    \\n    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\\n        self.network_architecture=network_architecture\\n        self.transfer_fct=transfer_fct        \\n        self._create_network()\\n        \\n    def _create_network(self):\\n        \\n        #initialize weights and biases \\n        network_weights = self._initialize_weights(**self.network_architecture)\\n        \\n    \\n    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\\n        all_weights = dict()\\n        all_weights[\"weights_recog\"] = {\\n            \\'h1\\': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\\n            \\'h2\\': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\\n            \\'out_mean\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\\n            \\'out_log_sigma\\' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \\n\\n        all_weights[\"biases_recog\"]={\\n            \\'b1\\': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\\n            \\'b2\\': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\\n            \\'out_mean\\': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\\n            \\'out_log_sigma\\': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\\n\\n        return all_weights\\n        \\n    def _recognition_network(self,weights,biases):\\n\\n        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights[\\'h1\\']),biases[\\'b1\\']))\\n        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights[\\'h2\\']),biases[\\'b2\\']))\\n\\n        z_mean = tf.add(tf.matmul(layer_2,weights[\\'out_mean\\']),biases[\\'out_mean\\'])\\n        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights[\\'out_log_sigma\\']),biases[\\'out_log_sigma\\'])\\n\\n        return (z_mean,z_log_sigma_sq)\\n        \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "network_architecture = dict(n_hidden_recog_1 = 500,\n",
    "     n_hidden_recog_2 = 500,\n",
    "     n_input = 20,\n",
    "     n_z = 20)\n",
    "     \n",
    "def xavier_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "class VariationalAutoencoder(object):\n",
    "    \n",
    "    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\n",
    "        self.network_architecture=network_architecture\n",
    "        self.transfer_fct=transfer_fct        \n",
    "        self._create_network()\n",
    "        \n",
    "    def _create_network(self):\n",
    "        \n",
    "        #initialize weights and biases \n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "    \n",
    "    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights[\"weights_recog\"] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\n",
    "            'out_mean' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\n",
    "            'out_log_sigma' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \n",
    "\n",
    "        all_weights[\"biases_recog\"]={\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    def _recognition_network(self,weights,biases):\n",
    "\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights['h1']),biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights['h2']),biases['b2']))\n",
    "\n",
    "        z_mean = tf.add(tf.matmul(layer_2,weights['out_mean']),biases['out_mean'])\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights['out_log_sigma']),biases['out_log_sigma'])\n",
    "\n",
    "        return (z_mean,z_log_sigma_sq)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 52\n",
    "input_size = 61\n",
    "hidden_size=20\n",
    "\n",
    "# our [486, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "inputs = tf.placeholder(tf.float32,[batch_size,max_char_len,input_size])\n",
    "inputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "_inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_char_len,name='char_array')\n",
    "_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='word_array')\n",
    "mean_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='mean_array')\n",
    "sigma_ta = tf.TensorArray(size=max_char_len, dtype=tf.float32,name='sigma_array')\n",
    "word_pos = tf.placeholder(tf.float32,[batch_size,max_char_len])\n",
    "word_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "# create loop_fn for raw_rnn\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "    if cell_output is None:  # time = 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "        sample_loop_state = output_ta\n",
    "        mean_loop_state = mean_ta\n",
    "        sigma_loop_state = sigma_ta\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\n",
    "    else:\n",
    "        word_slice = tf.tile(word_pos[:,time],[20])\n",
    "        word_slice = tf.reshape(word_slice,[52,20])\n",
    "        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "        \n",
    "        #reparametrization\n",
    "        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*hidden_size)\n",
    "        z_mean = z_concat[:,:20]\n",
    "        z_log_sigma_sq =  z_concat[:,20:40]\n",
    "        eps = tf.random_normal((batch_size,hidden_size),0,1,dtype=tf.float32)\n",
    "        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)),eps))\n",
    "        \n",
    "        z_sample = tf.multiply(z_sample,word_slice)\n",
    "        z_mean = tf.multiply(z_mean,word_slice)\n",
    "        z_log_sigma_sq = tf.multiply(z_log_sigma_sq,word_slice)\n",
    "        \n",
    "        next_cell_state = z_sample\n",
    "        sample_loop_state = loop_state[0].write(time - 1, next_cell_state)\n",
    "        mean_loop_state = loop_state[1].write(time - 1, z_mean)\n",
    "        sigma_loop_state = loop_state[2].write(time - 1, z_log_sigma_sq)\n",
    "        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "        \n",
    "        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "    elements_finished = (time >= max_char_len-1)\n",
    "    next_input = _inputs_ta.read(time)\n",
    "\n",
    "    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "outputs_ta, final_state, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "word_state_out = word_state[0].stack()\n",
    "mean_state_out = word_state[1].stack()\n",
    "sigma_state_out = word_state[2].stack()\n",
    "outputs = outputs_ta.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 486\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "        \n",
    "        for bt in range(1):\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_pos[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs, final_state, word_state_out,\n",
    "                                                                   mean_state_out,sigma_state_out],\n",
    "                                                       feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90177065 -0.76366216 -0.46059814  0.82181251  0.72128481 -2.41631293\n",
      "  0.39978823 -0.20097497 -0.47727609 -0.16006508  0.15669025 -0.16218063\n",
      " -2.03527522 -0.91288054  0.02005177 -0.23342226  0.23463117  2.32218027\n",
      " -0.12064796 -0.39263052  1.32413161  1.58943963 -0.12697785 -1.21855378\n",
      " -0.80298966  0.03696699  0.33881888 -0.74054068 -0.37150598  0.76808202\n",
      "  0.6204527  -0.67678624 -1.01259768 -1.08227277 -2.28174448  0.22970393\n",
      "  0.47315291 -1.18524146 -0.93505359  0.83128446 -1.08167946 -0.32876056\n",
      "  1.17995727 -0.45850673 -0.14342678 -0.40582576 -1.04825127  0.49350843\n",
      " -0.89809275 -0.91706252  0.62568593 -0.51209748]\n",
      "[ 0.20071873  0.75407177  0.81818324  1.0764184   0.66987795  0.61763489\n",
      "  0.91891837  1.07494164  0.3681511  -0.34788603 -0.64294749 -0.58652747\n",
      " -0.44313952 -0.97416162 -0.72422433 -0.62348872  0.61797816 -0.25447989\n",
      " -0.68625975 -0.08064329  1.03592956 -0.72895068  0.72128111  1.70032418\n",
      "  0.74682653 -0.10252037 -1.15162849  2.18138862  0.29951391 -0.52364594\n",
      " -1.73562717 -0.13509864  1.24729168  0.50194222  0.33908296  0.71817911\n",
      "  0.02608569  0.26486304 -0.58059847  1.02257872 -0.03909998 -0.34836605\n",
      "  0.10817565  1.86158383 -0.48530412 -1.6886102  -0.79386652  1.50477493\n",
      " -1.73605227  0.01660448  0.96917588  0.79476315]\n",
      "[-0.68280542  0.88159406 -0.43142274  2.06207204  0.46441472  0.76510471\n",
      " -0.81776881 -3.02025843  0.61716092  1.70369148  1.48214328 -0.43182084\n",
      "  1.88560009  1.69673181  0.64292574 -0.19767569  1.5240736  -1.17191601\n",
      " -1.59102392  0.34800318 -0.11322518 -0.16230699 -1.17244494  0.20099321\n",
      " -0.64166462 -1.78900278  0.5677073   0.59143806 -0.61570483 -0.89451486\n",
      " -1.27625847  0.57389814 -1.43652403 -1.09899235  0.58326137  0.89224792\n",
      "  1.24864161  0.68186021  1.32454324 -1.02544212 -1.06670225  1.16823936\n",
      " -0.4144766   1.56775713  1.15268791 -1.19010234  0.19809206 -0.28520033\n",
      "  0.80020779 -0.1888331  -1.3266108   1.07759595]\n",
      "[ 0.41571379  2.05336356  0.09203862  0.84084988  0.48090634 -1.28732586\n",
      "  0.33427784 -0.71942055  1.60959089 -0.3415792  -0.25475237 -1.34069324\n",
      "  1.00716722 -0.53905374 -0.10702747  2.23470187 -0.79962146 -0.55495358\n",
      " -1.09639525 -0.20678109  2.19845366  0.01548599 -0.85252875  0.81685573\n",
      "  1.33133483  0.02981012  1.35507047  0.28356466  0.21310847  0.27367148\n",
      " -1.61844373  0.75493878 -0.82406592 -2.90214109 -0.4388839  -0.19587302\n",
      " -1.39185143 -0.46057057  0.69114798  0.31243455  0.17327826 -1.18406129\n",
      "  1.13773167 -0.40129128 -0.66129279  0.0044146  -0.6470744   0.51250803\n",
      "  0.69826347  1.47671306  1.46647847  0.17239808]\n",
      "[-0.17878948  1.3797158  -0.2646119   0.47596103 -1.74857843  0.0128688\n",
      " -0.6605916  -1.20567882  1.55574167  1.17805862  3.00850916  0.97871792\n",
      "  0.48277524  1.12175739 -0.3574155   0.84973431 -1.16696918  0.78089625\n",
      "  1.50507092 -0.47702822 -1.40693796 -0.09600966  0.46793261  2.06208801\n",
      "  0.25765753  0.63552719 -0.18876605  1.10568738 -1.07961094 -1.46989155\n",
      "  1.58835268  0.86001706 -0.98361814  0.38316557  0.54061329  1.22931707\n",
      " -1.54515266 -1.26086199  0.24770337 -0.13282678 -0.5565992   1.01859486\n",
      "  0.57510537 -0.18641979  0.82659709  0.19750451 -0.67386419  2.0685215\n",
      "  0.41250312  0.09215375 -1.10476637  0.08559348]\n",
      "[ 0.72699606 -1.33372724  0.7641651   0.07558879 -1.20256352  0.4558965\n",
      "  1.07471454  0.47825152  1.56926084  0.64082569 -0.72920656 -0.24792628\n",
      " -0.44083664 -1.7195946  -1.12934363  1.35324693 -0.54684985  1.55006492\n",
      " -0.26457125 -0.49712193 -0.62873238 -0.69455713  0.63450783  0.32814351\n",
      "  0.61586368  1.13202345 -1.04783666  0.21176305  1.11462319 -0.4773612\n",
      "  0.03518075  0.77006775 -1.59814644 -1.39296269  0.05088891  1.80616844\n",
      " -1.95607221  1.09681034 -0.09079445 -1.1465807   0.379291   -1.25684714\n",
      " -0.6074791   0.75350136  0.12483495 -0.37963828 -2.03390813 -0.90757507\n",
      " -2.03107381 -0.08766899 -0.9520539   0.34837836]\n",
      "[-2.65212011 -1.85646725 -1.57098961  0.7033127  -1.3257767   0.45349157\n",
      " -0.43994364  1.48188603  0.03073611 -0.6321286   0.40914032  0.64731443\n",
      "  1.88024366  0.37421286 -0.51063353  0.87630242  0.67712992  0.44550452\n",
      "  0.41581136 -1.70869875 -1.81438935 -0.93471622  0.52348524  0.50244522\n",
      " -1.26636028 -0.01157989  0.85423523 -0.54534584 -0.3635585   0.37338504\n",
      "  0.26329842  0.560974    1.42564726 -1.64763796  0.46691063 -1.27432358\n",
      "  0.15573002 -0.8209303  -0.0129968  -1.17902398  0.28631756 -1.08011246\n",
      " -0.87995958  0.73864949  0.07998749 -0.19866334  0.33711818  2.03999519\n",
      " -0.55251491 -1.16764545 -0.58264917 -1.65961766]\n",
      "[ 0.05431117 -0.29561755 -0.48916298  1.65184999 -0.95006222 -0.63996035\n",
      "  0.04353805  0.49285412  0.10240463  1.08872294  1.22962284  0.090276\n",
      " -0.86366272 -0.63193846 -0.50884283  0.10290739 -0.58876699 -1.03347504\n",
      "  1.9885639   1.18465185 -0.19538814 -0.39802298 -0.8874808   0.22338161\n",
      " -0.30529845 -0.83683878  0.03904843  1.20165277  2.05557108  0.73268044\n",
      "  0.79049736  1.36143637 -1.98223305 -0.02846385 -0.59449524 -0.34035209\n",
      " -0.41942626  0.71059799  2.10280442 -0.27086711 -0.51395732 -0.06295376\n",
      " -2.22862077  0.7494005   0.02799451 -0.12922148  0.59075832 -1.42514169\n",
      " -0.85332513  0.29600343  0.84523374 -0.97928959]\n",
      "[-0.18769993 -0.20298177 -1.14443958 -0.07620508  0.50862396  0.78920782\n",
      "  2.31945348 -1.10497594  1.01325965  0.30591035  1.89442849  0.30084521\n",
      " -1.12809682 -1.15404856 -1.10604548  2.03004479  1.5669328  -1.42164516\n",
      " -1.43736076  0.28938246  0.44555578  0.88936788  1.04286504 -2.05072474\n",
      "  2.03787518 -1.71050131 -0.02579388 -0.04581551 -0.12184283  0.27760541\n",
      "  1.167068   -0.58911622  0.76868135 -0.28714904  1.39851403 -0.41905627\n",
      "  0.60749036 -0.12900397  0.04149317  0.94899976 -1.23849964  1.18141997\n",
      " -0.25317237  0.13890015  0.44080573 -1.02434933  0.69975656 -0.29756764\n",
      " -0.2556527   1.60202408  0.71000081 -0.19234207]\n",
      "[ 0.37544888  0.42479694 -0.15308629 -0.82602626  0.59402394  0.44730717\n",
      " -1.43862045 -1.37805748 -0.40338382  0.53057128 -0.21311276 -0.52887976\n",
      "  0.58191299  0.75696248  1.14038742 -0.70198351  0.9214288  -0.81254399\n",
      " -1.56218481  1.51777244  0.31829712 -1.66468358 -1.0926944   0.79554021\n",
      " -0.95381951 -0.38611007  1.03431666  1.8778286  -0.34564966 -1.07870173\n",
      " -0.82354516  0.42311159 -0.16682296  1.0211643  -0.94294107 -0.78147715\n",
      " -1.10557199 -0.0495395  -2.11293888 -0.1634686   0.31256798 -1.16933692\n",
      "  1.10914385 -1.10607028 -0.33043939  1.75722301  1.18038857 -1.61887622\n",
      "  0.66156965  0.32036018  1.00100684  0.10843685]\n",
      "[-0.22811154  1.0373652  -0.45562974 -1.01718402 -0.21824557 -1.49216568\n",
      " -1.32090068 -1.15453827  0.31914634  0.74376112 -0.36140391 -0.53428257\n",
      "  0.0662517   0.0152951  -0.70508707 -1.1185776  -0.1504593  -0.61247647\n",
      " -0.68222839 -1.09807539  0.4057889   0.81215459 -0.75525391  0.41826385\n",
      "  0.98540348  0.16210848  0.78191578 -0.72300637 -0.22953467  1.27419209\n",
      " -0.97142398 -0.80054504  0.52027422 -1.39273059  0.63479197  0.74624586\n",
      " -0.7284503   0.13556533 -1.15355229 -0.96618152  1.55979097  1.35208023\n",
      " -0.31457016 -0.21781978  0.93531811 -0.90548998  0.27134776  1.18412757\n",
      " -0.82994509 -0.81691605  0.97656745  0.00579778]\n",
      "[-0.14194943 -0.35738862 -0.42293859 -2.56069756  0.71598619  1.26347649\n",
      "  1.25039482 -0.92396212 -0.2429615   0.41917956  1.74848807 -0.70820093\n",
      "  2.03146052 -1.07897508 -0.2543532   0.64803213 -0.94083089 -1.28336048\n",
      " -1.91795993 -0.16204235  0.34141621 -0.51099825  0.79325521  1.18281698\n",
      "  1.25134969  0.72986162 -0.40785542 -0.91568035 -0.90528822 -0.84915733\n",
      "  0.15419123 -0.75443697 -1.58318067 -0.83940589  0.80679935 -0.82462513\n",
      " -0.93861479  0.03144509  0.51795959 -1.54755604  0.14359555 -0.18520184\n",
      "  0.37788138 -3.14459682  1.02442396 -1.1159699   0.69705582  1.05914474\n",
      " -0.22773217  1.46184909  0.90095615  2.28053999]\n",
      "[ 0.81458551 -0.17677689  0.36768973  0.88290048 -0.48252675  0.26733747\n",
      "  0.11414282  1.30961692  0.45940524 -0.21929893  0.96185315 -0.00641515\n",
      "  0.21082565 -0.86150903  0.00848583 -0.64933425  0.7040292   0.62137216\n",
      " -0.57447356 -0.20048164  0.22591297  0.82122874 -0.07335054  0.54097366\n",
      "  0.14479309  0.00957725  0.9320426  -0.31058875 -1.65807974 -1.70665741\n",
      "  1.10623968  1.1503495   0.86381853 -0.60636395  0.16866869 -0.15834963\n",
      " -0.87435895 -0.95615596 -1.05407333  0.14401779 -2.79590774  0.19310945\n",
      " -0.6942547  -0.10361991 -1.53031921  0.78586417 -1.12693477  0.01971718\n",
      "  0.50002462 -0.15597405 -0.47497603  0.50174892]\n",
      "[-1.18722379  0.03782129  1.0937916   0.71291095 -1.88058627  0.26942492\n",
      " -1.74358392 -1.27160943  2.18288803 -0.95397675  1.31174123  0.06162877\n",
      "  0.59347945 -0.20013963  0.02450247 -0.10081116  0.92981422 -0.17122442\n",
      "  0.67520338  0.06921007 -0.71541017  1.82419658 -0.30226469 -0.48823577\n",
      "  0.38047326  0.92200983 -1.49659288 -2.13554192 -0.3954767  -0.70096511\n",
      "  0.27961159 -0.59876037  1.38135362  0.22043239  2.39587426  0.40484178\n",
      " -0.27229047  0.05126614  1.4313767  -1.47683501  0.8794347   0.57311165\n",
      "  1.03387904 -0.12970895 -0.04866631 -1.10729122  0.74407554  0.42374665\n",
      "  0.1835728  -1.2057569   0.17720212  0.41049293]\n",
      "[-0.11021993  1.20262253 -0.43606716  0.22445337  0.14053866  0.33228034\n",
      " -1.09149945 -0.33227444  1.84628737  0.75401694 -0.55910718  0.39490196\n",
      "  0.58318239 -0.99367303  0.65544832 -0.22264902 -1.00375676  0.86790293\n",
      "  0.39022678 -2.00740457  0.16485789  0.77119225  0.82061207 -0.91754633\n",
      "  1.06182814 -0.28565407 -1.54970455 -1.78653181  2.26552701 -1.28142273\n",
      " -0.83118027  0.52696896  0.35673329  1.53997397 -0.00460632 -1.41869318\n",
      "  0.193435   -0.25955483 -1.50964284  0.77945483 -0.72671777 -0.69168758\n",
      "  0.51207179 -0.87835968 -0.76706642 -2.13438249  0.67730087  0.24475801\n",
      "  0.52301133 -1.52822995 -0.68794233  0.04318152]\n",
      "[  3.15022230e-01  -1.56358826e+00   2.31695652e-01   2.63379961e-01\n",
      "   3.97970051e-01   1.36228919e+00   3.79770517e-01  -5.70513785e-01\n",
      "   5.85656524e-01   9.34717000e-01  -4.99584764e-01   1.60352623e+00\n",
      "   5.62686980e-01  -4.89917725e-01   6.58914521e-02   6.90635860e-01\n",
      "   2.91808248e-01  -5.86357176e-01   3.44020277e-01   1.88525379e-01\n",
      "  -1.33552802e+00   1.62684774e+00   6.13664389e-01  -9.05110478e-01\n",
      "  -2.01588988e-01   3.97190183e-01   2.29044072e-03   1.05307922e-01\n",
      "   1.58286011e+00  -2.48227626e-01  -2.21395239e-01   1.70976296e-01\n",
      "   4.80549425e-01   2.99887824e+00   9.39920366e-01  -6.09803535e-02\n",
      "   2.31793165e+00  -3.45373482e-01   8.13724458e-01   2.91174352e-01\n",
      "  -4.13378775e-01  -4.25094366e-02  -9.29503679e-01  -4.50340211e-01\n",
      "  -2.63052344e+00  -1.14499462e+00   1.19939037e-01   5.88151991e-01\n",
      "   3.91058832e-01  -5.92938103e-02  -5.89434683e-01  -2.26728678e-01]\n",
      "[ -1.23339343e+00   6.07270122e-01  -1.99861217e+00   1.00210503e-01\n",
      "  -1.05337548e+00  -1.23558629e+00  -4.44368035e-01  -4.98545587e-01\n",
      "   3.23456526e-01  -1.64477575e+00   1.17767072e+00   5.85842252e-01\n",
      "   6.45016730e-01  -6.89741194e-01  -1.13535655e+00  -5.68654239e-01\n",
      "   1.12211549e+00  -1.65467179e+00  -6.71242833e-01   1.11967885e+00\n",
      "   1.13226652e+00  -1.41677821e+00  -1.00647247e+00   2.00727224e-01\n",
      "  -1.55969691e+00  -5.92116773e-01  -3.68129879e-01   2.62605818e-03\n",
      "  -1.51738501e+00  -6.31826937e-01   6.17944956e-01   8.71325374e-01\n",
      "   1.00890957e-02   2.49570084e+00  -5.92283785e-01  -1.94933657e-02\n",
      "  -1.01185568e-01   3.91597778e-01   1.43808758e+00  -1.10133879e-01\n",
      "   1.21180904e+00   2.13578343e+00  -4.25485969e-01   4.65854019e-01\n",
      "  -1.53437600e-01   6.36420369e-01   1.45453441e+00   1.54647410e+00\n",
      "  -1.52768660e+00   1.47317636e+00  -2.88303590e+00  -1.52936265e-01]\n",
      "[-1.58913183  1.36779869 -0.2480368  -0.18187939 -2.24969387  0.3418217\n",
      " -1.93645978 -1.04395247 -0.82016075  0.96679229 -0.43186915  0.31129339\n",
      "  0.271106   -0.60979611  0.43147701  0.04477281  1.26277423  0.8436203\n",
      " -0.34961468 -0.72187078 -0.03497411  0.68772411  0.48958978 -0.7682268\n",
      "  0.79226559  0.0375482  -0.80339003  0.65084857 -0.76491696 -0.96789753\n",
      "  0.94576359  1.24759126  0.53918391 -1.80285847  0.03856625 -0.02906899\n",
      "  0.70818681 -1.33785629  0.33079761 -0.8759765   0.16466543  0.50663286\n",
      "  0.35845599 -0.10538083 -1.80666435 -0.5879451   1.32823086  0.27102524\n",
      "  0.65496475  0.27171636 -0.05939111 -0.51078326]\n",
      "[-2.17947745 -1.14038146 -0.20159853 -1.75510895  1.43079436  0.59211838\n",
      " -0.42940709 -1.20723665  0.11690519 -0.43018633  0.07699938 -1.80900526\n",
      " -0.77597743  0.26778674 -1.6034292  -1.80741131 -1.18618166 -0.80439007\n",
      "  0.77141809 -0.12601048  0.34036762 -1.67078233  1.24812591  1.23296452\n",
      "  1.29414451 -0.49326509 -0.04627919  0.18142234 -1.05750859 -0.10646252\n",
      "  1.21897769 -0.50424743  0.05185205  0.63639152  0.12672475  1.51445735\n",
      "  0.45378053  1.05906177 -0.24205165 -0.48633742  1.26039171  0.96161544\n",
      " -1.00331688 -1.85589504 -0.06678373 -0.83924979 -0.28330529  1.01405108\n",
      "  0.07277954  1.25960422  0.87105703 -0.05191138]\n",
      "[-0.35721919  0.77885711 -0.00665789  0.38503724  1.62626839  0.71418488\n",
      "  1.05891764  1.00958371 -0.87484252 -0.44310722  1.5815984  -0.19962867\n",
      " -0.53254944  0.81699944  0.0035754   1.24923539  1.72443128 -0.4393101\n",
      "  0.72115242  0.83771962  1.29752338 -1.16874361  0.04698178  1.98585761\n",
      "  1.01933789  2.76259613  1.68862104  0.47863981 -0.68790603  1.58272767\n",
      "  0.26215768 -1.24992979 -0.63768649 -0.48550233 -1.24109888 -2.49759698\n",
      "  0.32661325  1.21410573  0.14551933  0.46391228 -0.25271702 -0.44618091\n",
      " -0.39368379 -0.37180358  1.70997024 -0.00695219  1.01529491  0.44528288\n",
      "  0.75696915 -1.0720762  -0.56713712  0.04487289]\n"
     ]
    }
   ],
   "source": [
    "#print(word_state.shape)\n",
    "#print(mean_state.shape)\n",
    "#print(sigma_state.shape)\n",
    "#word_state = word_state[1]\n",
    "#print(word_state.shape)\n",
    "for i in range(20):\n",
    "    print(word_state[0,:,i])\n",
    "    #print(mean_state[0,:,i])\n",
    "    #print(sigma_state[0,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
