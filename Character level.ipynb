{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "3370\n",
      "3761\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"LSTM/LSTM/simple-examples/data/ptb.test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'mr.', '>', 'is', 'chairman', 'of', '>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "a\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "[\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"z\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "29099\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "\n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor(arg):\n",
    "    return tf.convert_to_tensor(arg,dtype=tf.int32)\n",
    "\n",
    "def embed_producer(sentences):\n",
    "    max_char_len = 486\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        prev = 0\n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        \n",
    "        \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n",
      "[   1.    6.   15.   17.   20.   27.   29.   35.   43.   48.   49.   56.\n",
      "   58.   67.   72.   78.   87.   90.   92.   97.   99.  100.  101.  103.\n",
      "  104.  105.  106.  111.  113.  116.  122.  127.  132.  139.  141.  145.\n",
      "  151.  162.  163.  164.  166.  167.  168.  170.  173.  180.  187.  188.\n",
      "  198.  202.  208.  215.  217.  220.  222.  225.  226.  227.  229.  230.\n",
      "  231.  236.  238.  245.  250.  260.  263.  264.  265.  267.  268.  269.\n",
      "  274.  275.  280.  288.  295.  297.  300.  307.  316.  317.  323.  324.]\n"
     ]
    }
   ],
   "source": [
    "data,word_loc_all = embed_producer(sentences)\n",
    "print(data[0][1])\n",
    "print(word_loc_all[29099])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.   7.   8.   9.  14.  17.  21.  25.  28.  33.  35.  36.  48.  56.  60.\n",
      "  61.  62.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[ 1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "eow_pos = np.zeros((len(sentences),max_char_len))\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(max_word_length):\n",
    "        eow_pos[i,int(word_loc_all[i,j])] = 1\n",
    "        \n",
    "print(word_loc_all[1])\n",
    "print(eow_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "network_architecture = dict(n_hidden_recog_1 = 500,\n",
    "     n_hidden_recog_2 = 500,\n",
    "     n_input = 20,\n",
    "     n_z = 20)\n",
    "     \n",
    "def xavier_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    high = constant*np.sqrt(6.0/(fan_in+fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval=low,maxval=high,dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "class VariationalAutoencoder(object):\n",
    "    \n",
    "    def __init__(self,network_architecture,transfer_fct=tf.nn.softplus):\n",
    "        self.network_architecture=network_architecture\n",
    "        self.transfer_fct=transfer_fct        \n",
    "        self._create_network()\n",
    "        \n",
    "    def _create_network(self):\n",
    "        \n",
    "        #initialize weights and biases \n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "        \n",
    "    \n",
    "    def _initialize_weights(self,n_hidden_recog_1,n_hidden_recog_2, n_input,n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights[\"weights_recog\"] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input,n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1,n_hidden_recog_2)),\n",
    "            'out_mean' : tf.Variable(xavier_init(n_hidden_recog_2,n_z)),\n",
    "            'out_log_sigma' : tf.Variable(xavier_init(n_hidden_recog_2,n_z))} \n",
    "\n",
    "        all_weights[\"biases_recog\"]={\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1],dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2],dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z],dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    def _recognition_network(self,weights,biases):\n",
    "\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x,weights['h1']),biases['b1']))\n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1,weights['h2']),biases['b2']))\n",
    "\n",
    "        z_mean = tf.add(tf.matmul(layer_2,weights['out_mean']),biases['out_mean'])\n",
    "        z_log_sigma_sq = tf.add(tf.matmul(layer_2,weights['out_log_sigma']),biases['out_log_sigma'])\n",
    "\n",
    "        return (z_mean,z_log_sigma_sq)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# our [486, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "inputs = tf.placeholder(tf.float32,[batch_size,max_char_len,input_size])\n",
    "inputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "_inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_char_len)\n",
    "_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "output_ta = tf.TensorArray(size=max_word_length, dtype=tf.int32)\n",
    "word_pos = tf.placeholder(tf.float32,[batch_size,max_char_len])\n",
    "word_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "# create loop_fn for raw_rnn\n",
    "def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "    if cell_output is None:  # time = 0\n",
    "        next_cell_state = cell.zero_state(batch_size, tf.float32)\n",
    "        next_loop_state = output_ta\n",
    "\n",
    "    else:\n",
    "        word_slice = tf.tile(word_pos[:,time+1],[20])\n",
    "        word_slice = tf.reshape(word_slice,[52,20])\n",
    "        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "\n",
    "        #reparametrization\n",
    "        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*hidden_size)\n",
    "        z_mean = z_concat[:,:20]\n",
    "        z_log_sigma_sq =  z_concat[:,20:40]\n",
    "        eps = tf.random_normal((batch_size,hidden_size),0,1,dtype=tf.float32)\n",
    "        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)),eps))\n",
    "\n",
    "        next_cell_state = z_sample\n",
    "        next_loop_state = loop_state.write(time - 1, next_cell_state)\n",
    "        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "    elements_finished = (time >= max_char_len)\n",
    "    next_input = _inputs_ta.read(time)\n",
    "\n",
    "    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "outputs_ta, final_state, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "outputs = outputs_ta.stack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x116a79710> has invalid type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>, must be a string or Tensor. (Can not convert a TensorArray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    276\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 277\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    278\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3411\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3412\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a TensorArray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-f62ffefd9029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mword_pos_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meow_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             outputs,final_state,word_state = sess.run([outputs, final_state, word_state],\n\u001b[0;32m---> 21\u001b[0;31m                                                        feed_dict={inputs:x,word_pos:word_pos_batch})\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1113\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    279\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    280\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    282\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x116a79710> has invalid type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>, must be a string or Tensor. (Can not convert a TensorArray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 486\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "\n",
    "        for bt in range(1):\n",
    "\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_pos[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state = sess.run([outputs, final_state, word_state],\n",
    "                                                       feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "            print(outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
