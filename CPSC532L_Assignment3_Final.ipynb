{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the data and extract it into `data/`. The dataset contains two files, both containing a single caption on each line. We should have 415,795 sentences in the training captions and 500 sentences in the validation captions.\n",
    "\n",
    "To download the data, run the following directly on your server: `wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a3_data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414143\n",
      "500\n",
      "A very clean and well decorated empty bathroom\n",
      "A panoramic view of a kitchen and all of its appliances.\n",
      "A blue and white bathroom with butterfly themed wall tiles.\n"
     ]
    }
   ],
   "source": [
    "# Load the data into memory.\n",
    "train_sentences = [line.strip() for line in open(\"data3/mscoco_train_captions.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"data3/mscoco_val_captions.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n",
    "\n",
    "All of the packages utilized should be installed on your Azure servers, however you will have to download an NLTK corpus. To do this, follow the instructions below:\n",
    "\n",
    "1. SSH to your Azure server\n",
    "2. Open up Python interpreter\n",
    "3. `import nltk`\n",
    "4. `nltk.download()`\n",
    "\n",
    "    You should now see something that looks like:\n",
    "\n",
    "    ```\n",
    "    >>> nltk.download()\n",
    "    NLTK Downloader\n",
    "    ---------------------------------------------------------------------------\n",
    "        d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
    "    ---------------------------------------------------------------------------\n",
    "    Downloader> \n",
    "\n",
    "    ```\n",
    "\n",
    "5. `d punkt`\n",
    "6. Provided the download finished successfully, you may now exit out of the Python interpreter and close the SSH connection.\n",
    "\n",
    "Please look through the functions provided below **carefully**, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'a', 'panoramic', 'view', 'of', 'a', 'kitchen', 'and', 'all', 'of', 'its', 'appliances', '.', '<EOS>']\n",
      "[2, 1, 0, 174, 6, 1, 63, 10, 319, 6, 157, 616, 4, 3]\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def numberize(sentence):\n",
    "    numberized = [word2index.get(word, 0) for word in sentence]\n",
    "    return numberized\n",
    "\n",
    "def one_hot(sentence):\n",
    "    numberized = numberize(sentence)\n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "\n",
    "print(sentences[1])\n",
    "print(numberize(sentences[1]))\n",
    "print(one_hot(sentences[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM (\n",
       "  (lstm): LSTM(1000, 300)\n",
       "  (out): Linear (300 -> 1000)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "input_size = vocabularySize\n",
    "output_size = vocabularySize\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, hidden,state):\n",
    "        output,(hidden,state) = self.lstm(inputs,(hidden,state))\n",
    "        output = self.out(output)\n",
    "        return output,hidden,state\n",
    "\n",
    "    def initHidden(self):\n",
    "        h = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        c = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return h.cuda(), c.cuda()\n",
    "\n",
    "\n",
    "decoder = DecoderLSTM(hidden_size, input_size, output_size) \n",
    "decoder.cuda()\n",
    "decoder.train()\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a Language Decoder\n",
    "\n",
    "We must now train the language decoder we implemented above. An important thing to pay attention to is the [inputs for an LSTM](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012292527940538196\n",
      "4.886725752624682\n",
      "4.336857649431972\n",
      "4.2257553900376585\n",
      "4.187116980613085\n",
      "4.148298711858418\n",
      "4.1136087421619445\n",
      "4.206291209293989\n",
      "4.223092802356398\n",
      "4.162142173205294\n",
      "4.1507513252129025\n",
      "4.11312288692043\n",
      "4.0729030377499145\n",
      "4.046462275332244\n",
      "4.240358991977747\n",
      "4.116486894174229\n",
      "4.058497967713027\n",
      "4.005690454113624\n",
      "4.034310622477259\n",
      "3.97756621327979\n",
      "4.0077196952222724\n",
      "4.055543205432031\n",
      "3.987977908147868\n",
      "3.979404921078065\n",
      "3.9741019444113546\n",
      "3.945411133030882\n",
      "3.9193164693212186\n",
      "4.044165189349143\n",
      "3.98813402596566\n",
      "3.9466712819218293\n",
      "3.936908598183193\n",
      "3.9422106237556442\n",
      "3.8951745515626026\n",
      "3.931647680826702\n",
      "4.112219685005499\n",
      "4.010604763434629\n",
      "3.9820996354137015\n",
      "3.9768720861739393\n",
      "3.9631242376099296\n",
      "3.9376604224580043\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(decoder,decoder_optimizer,criterion,embeddings): \n",
    "    \n",
    "    loss = 0\n",
    "    decoder_optimizer.zero_grad()\n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    \n",
    "    #use embeddings as target variable \n",
    "    target_variable = embeddings\n",
    "    nWords, VocSize = target_variable.shape\n",
    "    \n",
    "    decoder_input = torch.FloatTensor(target_variable[1]) \n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "    decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    #Without teacher forcing #ignore <SOS> #teaching it to break at <EOS>\n",
    "    for di in range(2,nWords):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        \n",
    "        temp_target = torch.FloatTensor(target_variable[di]) \n",
    "        temp_target = temp_target.unsqueeze(0).unsqueeze(0)  \n",
    "        decoder_target = Variable(temp_target).cuda()\n",
    "        decoder_target = decoder_target.long()\n",
    "        decoder_target = decoder_target.squeeze(0)\n",
    "        label = torch.max(decoder_target, 1)[1]\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        next_input = one_hot_embeddings[ni.cpu().numpy()]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "        loss += criterion(decoder_output.squeeze(0), label)\n",
    "          \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / (nWords - 1)\n",
    "    \n",
    "\n",
    "# Train the model and monitor the loss\n",
    "def trainIters(decoder, epochs, learning_rate):\n",
    "    \n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for sentence in sentences[:200000]:\n",
    "            \n",
    "            embeddings = one_hot(sentence)\n",
    "            loss = train(decoder, decoder_optimizer, criterion, embeddings)\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            if (count % 5000 == 0):\n",
    "                print(plot_loss_total/5000)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            count = count + 1\n",
    "         \n",
    "    \n",
    "    \n",
    "epochs = 1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "trainIters(decoder,epochs,learning_rate)  \n",
    "\n",
    "torch.save(decoder.state_dict(), './decoder.pth')\n",
    "print('training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder and test it with a few different starting words. This code will be fairly similar to your training function from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man of a a a a a a .\n",
      "man man of a a a a a . .\n",
      "woman man of a a a a a . .\n",
      "dog man of a a a a a . .\n"
     ]
    }
   ],
   "source": [
    "decoder.load_state_dict(torch.load('./decoder.pth'))\n",
    "softmax = nn.Softmax()\n",
    "#decoder.eval()\n",
    "\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "def inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    \n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    decoded_words = [init_word]\n",
    "    \n",
    "    ind = word2index.get(init_word, 0)\n",
    "    one_hot_vec = embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        #detect <EOS> \n",
    "        if (ni == 2):\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "\n",
    "        next_input = one_hot_embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "s = \" \"\n",
    "print(s.join(inference(decoder, init_word=\"the\")))\n",
    "print(s.join(inference(decoder, init_word=\"man\")))\n",
    "print(s.join(inference(decoder, init_word=\"woman\")))\n",
    "print(s.join(inference(decoder, init_word=\"dog\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The decoder is trained without teacher forcing and without any sort of assistance and MAP inference we could expect it to get stuck to a particular type of sequence like the one above.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building Language Decoder Sampling Inference\n",
    "\n",
    "We must now modify the method defined in part 3, to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the are this the a a boats about at <UNK>\n",
      "man <UNK> man people while pose black suit dirt 's hole covered distance <UNK>\n",
      "woman girl on <UNK> truck types event placed tree by grass of beach rain dress boy\n",
      "dog baby full gray an book dog a tall\n",
      " \n",
      "the car grass white with bedroom hillside a tall umbrella . a\n",
      "man brown <UNK> <UNK> an fence back pizza of can bridge\n",
      "woman small walks walking on soccer grass a road to suit\n",
      "dog brown trunk holding the bunch during pulled a zoo blue looking of\n",
      " \n",
      "the lush girl colored standing a a <UNK> chair area lawn\n",
      "man bunch dog empty city sand ties next for into pieces <UNK> other on can .\n",
      "woman black is lying shirt and dressed different ocean mountain background . a\n",
      "dog sitting are one walk women other water graze nearby station . big by park the picture . a\n",
      " \n",
      "the walks <UNK> its in set to it lamp himself . .\n",
      "man parked on <UNK> camera top before both grassy zoo other\n",
      "woman lies white dog on into a suit sand side mother he . <UNK> rocks and . small\n",
      "dog outdoor empty are an for ties he playing device\n",
      " \n",
      "the man of jumping drinks sit wood through a . . tall\n",
      "man baby and seen across many pen beach metal road down young\n",
      "woman family in be wall umbrella himself polar . a airport\n",
      "dog outside petting on small have around mouse\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from bisect import bisect\n",
    "from random import random\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "decoder.load_state_dict(torch.load('./decoder.pth'))\n",
    "\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "def sampling_inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    \n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    decoded_words = [init_word]\n",
    "\n",
    "    decoded_words = [init_word]\n",
    "    \n",
    "    ind = word2index.get(init_word, 0)\n",
    "    one_hot_vec = embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        p = decoder_output.data.squeeze().cpu().numpy()\n",
    "        \n",
    "        cdf = [p[0]]\n",
    "        for i in range(1, len(p)):\n",
    "            cdf.append(cdf[-1] + p[i])\n",
    "\n",
    "        ni = bisect(cdf,random())\n",
    "        \n",
    "            \n",
    "        #detect <EOS> \n",
    "        if (ni == 2):\n",
    "            #decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "        \n",
    "        next_input = one_hot_embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0))\n",
    "        decoder_input = decoder_input.cuda() \n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "s = \" \"\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"the\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"man\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"woman\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"dog\")))\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With sampling, the inference is not repetitive but neither is it sensible. The sampling helps the inference escape a overfitting sequence but the decoder is still not well trained enough.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLSTM (\n",
       "  (lstm): LSTM(1000, 300)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "input_size = vocabularySize\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs,hidden,state):\n",
    "        output,(hidden,state) = self.lstm(inputs,(hidden,state))\n",
    "        return output,hidden,state\n",
    "\n",
    "    def initHidden(self):\n",
    "        h = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        c = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return h.cuda(), c.cuda()\n",
    "        \n",
    "\n",
    "encoder = EncoderLSTM(input_size, hidden_size)\n",
    "encoder.cuda()\n",
    "encoder.train()\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Connecting Encoder to Decoder and Training End-to-End\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "It's likely that you'll be able to re-use most of your code from part 2. For our purposes, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012415499114990234\n",
      "4.395923924233522\n",
      "3.8804789565686595\n",
      "3.8117521413752575\n",
      "3.7369077205875603\n",
      "3.6265489139530023\n",
      "3.504738045248612\n",
      "3.525242896123824\n",
      "3.4492659143583793\n",
      "3.2708842698387404\n",
      "3.1814707714514996\n",
      "3.0152420302247838\n",
      "2.89141593246968\n",
      "2.752547861714314\n",
      "2.8693497255925395\n",
      "2.651424077232\n",
      "2.5234132797761375\n",
      "2.385096574447404\n",
      "2.350763037304252\n",
      "2.2259060771188692\n",
      "2.2155633906129037\n",
      "2.2331759170922063\n",
      "2.1212107715304764\n",
      "2.0649700116268157\n",
      "1.9905722349000823\n",
      "1.927227783126948\n",
      "1.8486965851818027\n",
      "1.9533152385798818\n",
      "1.8582603858281683\n",
      "1.7862792856896434\n",
      "1.7453329136336375\n",
      "1.688983353553361\n",
      "1.6122121211462053\n",
      "1.63759907672229\n",
      "1.8667013490501907\n",
      "1.7437227918804892\n",
      "1.6763909112494053\n",
      "1.6065134934827499\n",
      "1.5624669757722371\n",
      "1.5101744062591578\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(embeddings, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion,hidden_states,max_length=maxSequenceLength):\n",
    "    \n",
    "    encoder_hidden,encoder_state = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = embeddings\n",
    "    target_variable = embeddings\n",
    "    nWords, VocSize = target_variable.shape\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(nWords,1, hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() \n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(1,nWords):\n",
    "        \n",
    "        encoder_input = torch.FloatTensor(input_variable[ei]) \n",
    "        encoder_input = encoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "        encoder_input = Variable(encoder_input).cuda()\n",
    "        \n",
    "        encoder_output, encoder_hidden,encoder_state = encoder(encoder_input,encoder_hidden,encoder_state)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "    \n",
    "    \n",
    "    ind = word2index.get(\"<SOS>\", 0)\n",
    "    one_hot_vec = one_hot_embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    hidden_states.append(encoder_hidden.squeeze().cpu().data.numpy()) \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    #Without teacher forcing\n",
    "    for di in range(1,nWords):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                              encoder_outputs[di].unsqueeze(0))\n",
    "        \n",
    "        temp_target = torch.FloatTensor(target_variable[di]) \n",
    "        temp_target = temp_target.unsqueeze(0).unsqueeze(0)  \n",
    "        decoder_target = Variable(temp_target).cuda()\n",
    "        decoder_target = decoder_target.long()\n",
    "        decoder_target = decoder_target.squeeze(0)\n",
    "        label = torch.max(decoder_target, 1)[1]\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        next_input = one_hot_embeddings[ni.cpu().numpy()]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "        loss += criterion(decoder_output.squeeze(0), label)\n",
    "            \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / nWords,hidden_states\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, epochs, learning_rate):\n",
    "    \n",
    "    hidden_states = []\n",
    "    plot_loss_total = 0  \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for sentence in sentences[:200000]:\n",
    "            \n",
    "            embeddings = one_hot(sentence)\n",
    "            loss,hidden_states = train(embeddings, encoder, decoder, encoder_optimizer,\n",
    "                                       decoder_optimizer,criterion,hidden_states)\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            if (count % 5000 == 0):\n",
    "                print(plot_loss_total/5000)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "        \n",
    "        np.save(open('outputs/hidden_states_train'+str(epoch), 'wb+'), hidden_states)    \n",
    "           \n",
    "\n",
    "        \n",
    "epochs = 1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "trainIters(encoder,decoder,epochs,learning_rate)\n",
    "\n",
    "torch.save(encoder.state_dict(), './encoder.pth')\n",
    "torch.save(decoder.state_dict(), './decoder2.pth')\n",
    "print('training done')   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It can be seen that with end to end training the loss values are lower than the previous cases with only the decoder trained.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'a', 'man', 'speaking', 'into', 'a', 'microphone', 'on', 'a', 'stage', 'with', 'a', 'bicycle', 'and', 'dressed', 'in', 'cyclist', 'gear', '.', '<EOS>']\n",
      "[3, 1, 12, 822, 86, 1, 608, 6, 1, 819, 9, 1, 459, 10, 224, 8, 901, 380, 4, 2]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "cc = SmoothingFunction()\n",
    "\n",
    "sentences = val_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "#bleu function with reweighting\n",
    "def bleu(reference_sentence, predicted_sentence):\n",
    "    return sentence_bleu([reference_sentence], predicted_sentence,smoothing_function=cc.method4)\n",
    "\n",
    "print(sentences[1])\n",
    "print(numberize(sentences[1]))\n",
    "print(one_hot(sentences[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence,encoder,decoder,hidden_states,max_length=maxSequenceLength):\n",
    "    \n",
    "    encoder_hidden,encoder_state = encoder.initHidden()\n",
    "    \n",
    "    embeddings = one_hot(sentence)\n",
    "    input_variable = embeddings\n",
    "    target_variable = embeddings\n",
    "    nWords, VocSize = target_variable.shape\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(nWords,1, hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() \n",
    "    \n",
    "    for ei in range(1,nWords):\n",
    "        \n",
    "        encoder_input = torch.FloatTensor(input_variable[ei]) \n",
    "        encoder_input = encoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "        encoder_input = Variable(encoder_input).cuda()\n",
    "        \n",
    "        encoder_output, encoder_hidden,encoder_state = encoder(encoder_input,encoder_hidden,encoder_state)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "        \n",
    "    \n",
    "    ind = word2index.get(\"<SOS>\", 0)\n",
    "    one_hot_vec = one_hot_embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    hidden_states.append(encoder_hidden.squeeze().cpu().data.numpy()) \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = [\"<SOS>\"]\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                              encoder_outputs[di].unsqueeze(0))\n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        \n",
    "        #detect <EOS> \n",
    "        if (ni == 2):\n",
    "            decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "\n",
    "        next_input = one_hot_embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "    \n",
    "    predicted = decoded_words\n",
    "    bleu_score = bleu(sentence,predicted)\n",
    "    return bleu_score,hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bleu score: 0.2261655668561503\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "hidden_states = []\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "encoder.load_state_dict(torch.load('./encoder.pth'))\n",
    "\n",
    "decoder.load_state_dict(torch.load('./decoder2.pth'))\n",
    "\n",
    "Total_bleu = 0 \n",
    "for sentence in sentences[:500]:\n",
    "    \n",
    "    blue_score,hidden_states = seq2seq_inference(sentence,encoder,decoder,hidden_states)\n",
    "    Total_bleu += blue_score\n",
    "\n",
    "\n",
    "np.save(open('outputs/hidden_states_val', 'wb+'), hidden_states)     \n",
    "print(\"Average bleu score:\",Total_bleu/500)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bleu score is realtively low and this could be because I did not train the model with the entire data but with only half of it. Also, one hot representation might not be the best form of representation for this task.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man and woman at a table with beer and wine\n",
      "a man is wearing a purple shirt and glasses\n",
      "\n",
      "A man speaking into a microphone on a stage with a bicycle and dressed in cyclist gear.\n",
      "A smiling, bespectacled young man leans wearing a tie with a t-shirt and jeans leans against a tree.\n",
      "\n",
      "Four horses are skattered around a small water hole.\n",
      "Five horse and buggies race while a crowd watches.\n",
      "\n",
      "A man and a young girl playing Wii\n",
      "a woman and a man are shaking hands\n",
      "\n",
      "A boat home sitting on a river bay.\n",
      "An acrobat rides a horse while spectators watch.\n",
      "\n",
      "Several Tim's of mints are stacked up with a bottle that has several  clipped roses inside\n",
      "Elephant raising it's trunk next to gate with a bench strapped to it's back\n",
      "\n",
      "Family at a pizza restaurant posing for a picture before meal.\n",
      "People walk outside with umbrellas, two men do not have umbrellas.\n",
      "\n",
      "Several mopeds are lined up along the side of a hotel parking lot.\n",
      "Wedding cake with figure of bride and groom on a silver platter.\n",
      "\n",
      "A young man appears to be taking a break from the waves.\n",
      "A young child catching a large red frisbee on a sidewalk.\n",
      "\n",
      "A baseball player standing next to home plate with a bat.\n",
      "Two little  zebras behind a fence with trees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now get nearest neighbors and print\n",
    "import math\n",
    "\n",
    "epoch = 0\n",
    "f_train = np.load(open('outputs/hidden_states_train'+str(epoch), 'rb'))\n",
    "f_val = np.load(open('outputs/hidden_states_val', 'rb'))\n",
    "\n",
    "\n",
    "for val_id in range(10):\n",
    "       \n",
    "    val_vec = f_val[val_id]\n",
    "    min_id = 0\n",
    "    min_dist = math.inf\n",
    "    for train_id in range(200000):\n",
    "        train_vec = f_train[train_id]\n",
    "        \n",
    "        dist = np.square(np.linalg.norm(val_vec-train_vec))\n",
    "        if (dist < min_dist):\n",
    "            min_dist = dist\n",
    "            min_id = train_id\n",
    "            \n",
    "           \n",
    "    \n",
    "    print(val_sentences[val_id])\n",
    "    print(train_sentences[min_id])\n",
    "    print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Effectiveness of word2vec\n",
    "\n",
    "We now repeat everything done above using word2vec embeddings in place of one-hot embeddings. This will require re-running steps 1-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.25918221 -0.83345079  0.55769587 ...,  1.30408192  0.19403522\n",
      "   1.05969226]\n",
      " [ 0.4817569  -0.73988849  0.32601929 ...,  1.22742546  0.57799971\n",
      "   0.68640846]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [-0.59881067  0.74625498 -0.05972543 ..., -0.17042953  0.39723638\n",
      "  -0.35443801]\n",
      " [-0.19171366  0.09861971  0.4687027  ...,  0.72048628 -0.25929466\n",
      "  -0.09999225]\n",
      " [ 0.46664095  0.47151706 -0.38249168 ...,  0.88390398 -0.55219585\n",
      "  -0.35060215]]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "def word2vec(sentence):\n",
    "    numberized = numberize(sentence)\n",
    "    \n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "print(word2vec(sentences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM (\n",
       "  (lstm): LSTM(300, 300)\n",
       "  (out): Linear (300 -> 1000)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEncodingSize = 300\n",
    "hidden_size = 300\n",
    "input_size = wordEncodingSize\n",
    "output_size = vocabularySize\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, hidden,state):\n",
    "        output,(hidden,state) = self.lstm(inputs,(hidden,state))\n",
    "        output = self.out(output)\n",
    "        return output,hidden,state\n",
    "\n",
    "    def initHidden(self):\n",
    "        h = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        c = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return h.cuda(), c.cuda()\n",
    "\n",
    "\n",
    "decoder = DecoderLSTM(hidden_size, input_size, output_size) \n",
    "decoder.cuda()\n",
    "decoder.train()\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010238756815592446\n",
      "4.054862071358708\n",
      "4.005052740334257\n",
      "3.9799438948169876\n",
      "3.9705545279096937\n",
      "3.9343653607996703\n",
      "3.8978847113871167\n",
      "4.037550494438426\n",
      "4.071012942297543\n",
      "4.00489829920382\n",
      "4.00068595684393\n",
      "3.958445666579537\n",
      "3.9350900953042203\n",
      "3.908114615435343\n",
      "4.094549600129489\n",
      "3.9515983262681855\n",
      "3.897029918298033\n",
      "3.850814563848063\n",
      "3.8879540896935354\n",
      "3.8382240496242934\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(decoder,decoder_optimizer,criterion,input_embeddings,target_embeddings): \n",
    "    \n",
    "    loss = 0\n",
    "    decoder_optimizer.zero_grad()\n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    \n",
    "    #use embeddings as target variable \n",
    "    target_variable = target_embeddings\n",
    "    input_variable = input_embeddings\n",
    "    nWords, VocSize = input_variable.shape\n",
    "    \n",
    "    decoder_input = torch.FloatTensor(input_variable[1]) \n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "    decoder_input = Variable(decoder_input).cuda()\n",
    "    \n",
    "    #Without teacher forcing #ignore <SOS> #teaching it to break at <EOS>\n",
    "    for di in range(2,nWords):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        \n",
    "        temp_target = torch.FloatTensor(target_variable[di]) \n",
    "        temp_target = temp_target.unsqueeze(0).unsqueeze(0)  \n",
    "        decoder_target = Variable(temp_target).cuda()\n",
    "        decoder_target = decoder_target.long()\n",
    "        decoder_target = decoder_target.squeeze(0)\n",
    "        label = torch.max(decoder_target, 1)[1]\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        next_input = w2v_embeddings[ni.cpu().numpy()]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "        loss += criterion(decoder_output.squeeze(0), label)\n",
    "          \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / (nWords - 1)\n",
    "    \n",
    "\n",
    "# Train the model and monitor the loss\n",
    "def trainIters(decoder, epochs, learning_rate):\n",
    "    \n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for sentence in sentences[:100000]:\n",
    "            \n",
    "            input_embeddings = word2vec(sentence)\n",
    "            target_embeddings = one_hot(sentence)\n",
    "            loss = train(decoder, decoder_optimizer, criterion, input_embeddings,target_embeddings)\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            if (count % 5000 == 0):\n",
    "                print(plot_loss_total/5000)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            count = count + 1\n",
    "         \n",
    "    \n",
    "    \n",
    "epochs = 1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "trainIters(decoder,epochs,learning_rate)  \n",
    "\n",
    "torch.save(decoder.state_dict(), './decoderW1.pth')\n",
    "print('training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the <UNK> <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK>\n",
      "man <UNK> <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK>\n",
      "woman <UNK> <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK>\n",
      "dog <UNK> <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "decoder.load_state_dict(torch.load('./decoderW1.pth'))\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "def inference(decoder, init_word, embeddings=w2v_embeddings, max_length=maxSequenceLength):\n",
    "    \n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    decoded_words = [init_word]\n",
    "    \n",
    "    ind = word2index.get(init_word, 0)\n",
    "    wordvec = embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(wordvec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        #detect <EOS> \n",
    "        if (ni == 2):\n",
    "            break\n",
    "        else:\n",
    "            #print(ni)\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "\n",
    "        next_input = embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "s = \" \"\n",
    "print(s.join(inference(decoder, init_word=\"the\")))\n",
    "print(s.join(inference(decoder, init_word=\"man\")))\n",
    "print(s.join(inference(decoder, init_word=\"woman\")))\n",
    "print(s.join(inference(decoder, init_word=\"dog\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting to a sequence without teacher forcing.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bird old a flock on standing with pair . the in on . . by it <UNK> and\n",
      "man sitting on grass are zoo on the is . . a <UNK> the\n",
      "woman bus bus pen together in horses a sitting the a . sky giraffe\n",
      "dog bus holds other . a blue field a <UNK> water long number <UNK> . .\n",
      " \n",
      "the red 's of a in the <UNK> all drinking <UNK> <UNK> on\n",
      "man red the the sitting bench in <UNK> sky around <UNK> in trees open\n",
      "woman <UNK> next the on of a <UNK> the <UNK> the\n",
      "dog shot different setting on walking in a a\n",
      " \n",
      "the\n",
      "man stopped around his very two with on birds the in street lights a <UNK>\n",
      "woman <UNK> trucks are a of parked railroad sheep a them hydrant . . . . bird concrete hydrant airport with <UNK>\n",
      "dog traffic man double a next benches <UNK> that <UNK> and bus bench a street front . . a pole\n",
      " \n",
      "the white <UNK> has of is near the a on other\n",
      "man driving near outside a around a a the traffic down <UNK>\n",
      "woman picture <UNK> on other a next giraffes sitting is home with\n",
      "dog red outside sits grazing a on to a by\n",
      " \n",
      "the <UNK> tower in a grazing together <UNK> antique at a <UNK> a . . .\n",
      "man airplane garden in along narrow placed area on at . <UNK> .\n",
      "woman plane fire has motorcycle a near another <UNK> <UNK> wooden a to the and the a\n",
      "dog going winter of in <UNK> the a a a group\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from bisect import bisect\n",
    "from random import random\n",
    "softmax = nn.Softmax()\n",
    "\n",
    "decoder.load_state_dict(torch.load('./decoderW1.pth'))\n",
    "\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "def sampling_inference(decoder, init_word, embeddings=w2v_embeddings, max_length=maxSequenceLength):\n",
    "    \n",
    "    decoder_hidden, decoder_state = decoder.initHidden()\n",
    "    decoded_words = [init_word]\n",
    "\n",
    "    decoded_words = [init_word]\n",
    "    \n",
    "    ind = word2index.get(init_word, 0)\n",
    "    one_hot_vec = embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,decoder_state)\n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        p = decoder_output.data.squeeze().cpu().numpy()\n",
    "        \n",
    "        cdf = [p[0]]\n",
    "        for i in range(1, len(p)):\n",
    "            cdf.append(cdf[-1] + p[i])\n",
    "\n",
    "        ni = bisect(cdf,random())\n",
    "        \n",
    "            \n",
    "        #detect <EOS> \n",
    "        if (ni == 2):\n",
    "            #decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "        \n",
    "        next_input = embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0))\n",
    "        decoder_input = decoder_input.cuda() \n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "s = \" \"\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"the\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"man\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"woman\")))\n",
    "    print(s.join(sampling_inference(decoder, init_word=\"dog\")))\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLSTM (\n",
       "  (lstm): LSTM(300, 300)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordEncodingSize = 300\n",
    "hidden_size = 300\n",
    "input_size = wordEncodingSize\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs,hidden,state):\n",
    "        output,(hidden,state) = self.lstm(inputs,(hidden,state))\n",
    "        return output,hidden,state\n",
    "\n",
    "    def initHidden(self):\n",
    "        h = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        c = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return h.cuda(), c.cuda()\n",
    "        \n",
    "\n",
    "encoder = EncoderLSTM(input_size, hidden_size)\n",
    "encoder.cuda()\n",
    "encoder.train()\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012406927490234375\n",
      "3.9714420115110762\n",
      "2.646813261747851\n",
      "2.089078832237761\n",
      "1.7201897547167064\n",
      "1.4526864824533725\n",
      "1.2431494200918727\n",
      "1.30844163656379\n",
      "1.2592034651597226\n",
      "1.0546482471747567\n",
      "0.9279688186010395\n",
      "0.7899628455017726\n",
      "0.7071595312610173\n",
      "0.6254463153885791\n",
      "0.7115813979200543\n",
      "0.5559409819726183\n",
      "0.4655920049473506\n",
      "0.3909160639053919\n",
      "0.3690152903705918\n",
      "0.3155109133435735\n",
      "0.3446520580577199\n",
      "0.3819416409768279\n",
      "0.3249696907668533\n",
      "0.2932152822757393\n",
      "0.2499133995378782\n",
      "0.22708863555338943\n",
      "0.2066523039587023\n",
      "0.27541561878267146\n",
      "0.2363468923944616\n",
      "0.20949713341679882\n",
      "0.18629429317420612\n",
      "0.1658664881581902\n",
      "0.1552178224388369\n",
      "0.1676084328808049\n",
      "0.20435986670918707\n",
      "0.19051033126424366\n",
      "0.1576427346991389\n",
      "0.14267350991140368\n",
      "0.13081848422938477\n",
      "0.12291006579770433\n",
      "0.20988723894583808\n",
      "0.20064539913245888\n",
      "0.16076310725382834\n",
      "0.13018595665315671\n",
      "0.11187859212777694\n",
      "0.11125913882631629\n",
      "0.13552578711613472\n",
      "0.30351470212680154\n",
      "0.21556650928767399\n",
      "0.1775215910564455\n",
      "0.1450113635452677\n",
      "0.12391603871965251\n",
      "0.10918006829480871\n",
      "0.17689436845050258\n",
      "0.18031066634622855\n",
      "0.13455145051496817\n",
      "0.10744771921816103\n",
      "0.08131107021428266\n",
      "0.07330786449038394\n",
      "0.06270578838220636\n",
      "0.12413421520587169\n",
      "0.09337294283646043\n",
      "0.06867066036858363\n",
      "0.057922513882183946\n",
      "0.0512153301124088\n",
      "0.04198210672299504\n",
      "0.12194537823479432\n",
      "0.11949166337997134\n",
      "0.07364533977970962\n",
      "0.05705044831189459\n",
      "0.041335302574992155\n",
      "0.03639405623127528\n",
      "0.031582003974210626\n",
      "0.0547012832922512\n",
      "0.047277847803822604\n",
      "0.03680293215347412\n",
      "0.03144767897337143\n",
      "0.02539050496352102\n",
      "0.02459513633240848\n",
      "0.03181826785659925\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(input_embeddings,target_embeddings, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion,hidden_states,max_length=maxSequenceLength):\n",
    "    \n",
    "    encoder_hidden,encoder_state = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_embeddings\n",
    "    target_variable = target_embeddings\n",
    "    nWords, VocSize = target_variable.shape\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(nWords,1, hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() \n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(1,nWords):\n",
    "        \n",
    "        encoder_input = torch.FloatTensor(input_variable[ei]) \n",
    "        encoder_input = encoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "        encoder_input = Variable(encoder_input).cuda()\n",
    "        \n",
    "        encoder_output, encoder_hidden,encoder_state = encoder(encoder_input,encoder_hidden,encoder_state)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "    \n",
    "    \n",
    "    ind = word2index.get(\"<SOS>\", 0)\n",
    "    wordvec = w2v_embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(wordvec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    hidden_states.append(encoder_hidden.squeeze().cpu().data.numpy()) \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    #Without teacher forcing\n",
    "    for di in range(1,nWords):\n",
    "        \n",
    "        decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                              encoder_outputs[di].unsqueeze(0))\n",
    "        \n",
    "        temp_target = torch.FloatTensor(target_variable[di])\n",
    "        temp_target = temp_target.unsqueeze(0).unsqueeze(0)  \n",
    "        decoder_target = Variable(temp_target).cuda()\n",
    "        decoder_target = decoder_target.long()\n",
    "        decoder_target = decoder_target.squeeze(0)\n",
    "        label = torch.max(decoder_target, 1)[1]\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "               \n",
    "        next_input = input_variable[di]  #teacher forcing \n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "        loss += criterion(decoder_output.squeeze(0), label)\n",
    "            \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / nWords,hidden_states\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, epochs, learning_rate):\n",
    "    \n",
    "    hidden_states = []\n",
    "    plot_loss_total = 0  \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for sentence in sentences[:400000]:\n",
    "            \n",
    "            input_embeddings = word2vec(sentence)\n",
    "            target_embeddings = one_hot(sentence)\n",
    "            loss,hidden_states = train(input_embeddings,target_embeddings, encoder, decoder, encoder_optimizer,\n",
    "                                       decoder_optimizer,criterion,hidden_states)\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "            if (count % 5000 == 0):\n",
    "                print(plot_loss_total/5000)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "        \n",
    "        np.save(open('outputs/hidden_states_train_W'+str(epoch), 'wb+'), hidden_states)    \n",
    "           \n",
    "\n",
    "        \n",
    "epochs = 1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "trainIters(encoder,decoder,epochs,learning_rate)\n",
    "\n",
    "torch.save(encoder.state_dict(), './encoderW.pth')\n",
    "torch.save(decoder.state_dict(), './decoderW2.pth')\n",
    "print('training done')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the training is done with almost the entire data and we can see a substantial decrease in the loss compared to the end-end model using one hot representation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'a', 'man', 'speaking', 'into', 'a', 'microphone', 'on', 'a', 'stage', 'with', 'a', 'bicycle', 'and', 'dressed', 'in', 'cyclist', 'gear', '.', '<EOS>']\n",
      "[2, 1, 12, 0, 152, 1, 0, 5, 1, 0, 9, 1, 352, 10, 371, 8, 0, 630, 4, 3]\n",
      "[[ 0.52435738 -1.17175257  1.43554449 ..., -0.22541249  0.08525697\n",
      "   0.51633179]\n",
      " [ 0.04788534 -1.0648185   0.52550334 ..., -0.19837394  0.5309515\n",
      "   0.47923851]\n",
      " [-0.6551522   0.16742826  0.79151756 ...,  0.71651602  2.03825712\n",
      "  -0.65796065]\n",
      " ..., \n",
      " [-0.17633598 -0.29771715 -0.1895007  ...,  0.25773174 -0.25257963\n",
      "  -1.73474598]\n",
      " [-0.45129138 -0.11723721 -0.30734822 ...,  0.33252311  0.50422186\n",
      "   0.48540848]\n",
      " [-0.69795328  0.94974667 -0.46290016 ...,  0.17884381  0.57679081\n",
      "   0.35002416]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "cc = SmoothingFunction()\n",
    "\n",
    "sentences = val_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "#bleu function with reweighting\n",
    "def bleu(reference_sentence, predicted_sentence):\n",
    "    return sentence_bleu([reference_sentence], predicted_sentence,smoothing_function=cc.method4)\n",
    "\n",
    "print(sentences[1])\n",
    "print(numberize(sentences[1]))\n",
    "print(word2vec(sentences[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence,encoder,decoder,hidden_states,max_length=maxSequenceLength):\n",
    "    \n",
    "    encoder_hidden,encoder_state = encoder.initHidden()\n",
    "    \n",
    "    input_variable = word2vec(sentence)\n",
    "    #target_variable = one_hot(sentence)\n",
    "    nWords, VocSize = input_variable.shape\n",
    "    \n",
    "    encoder_outputs = Variable(torch.zeros(nWords,1, hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() \n",
    "    \n",
    "    for ei in range(1,nWords):\n",
    "        \n",
    "        encoder_input = torch.FloatTensor(input_variable[ei]) \n",
    "        encoder_input = encoder_input.unsqueeze(0).unsqueeze(0)  \n",
    "        encoder_input = Variable(encoder_input).cuda()\n",
    "        \n",
    "        encoder_output, encoder_hidden,encoder_state = encoder(encoder_input,encoder_hidden,encoder_state)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "        \n",
    "    \n",
    "    ind = word2index.get(\"<SOS>\", 0)\n",
    "    one_hot_vec = w2v_embeddings[ind]\n",
    "    decoder_input = torch.FloatTensor(one_hot_vec)\n",
    "    decoder_input = decoder_input.unsqueeze(0).unsqueeze(0) \n",
    "    decoder_input = Variable(decoder_input).cuda() \n",
    "    \n",
    "    hidden_states.append(encoder_hidden.squeeze().cpu().data.numpy()) \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = [\"<SOS>\"]\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        \n",
    "        if (di<nWords):\n",
    "            decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                              encoder_outputs[di].unsqueeze(0))\n",
    "        else: \n",
    "            decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                              encoder_outputs[nWords-1].unsqueeze(0))\n",
    "            \n",
    "        decoder_output = softmax(decoder_output.squeeze(0))\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        \n",
    "        #detect <EOS> \n",
    "        if (ni == 3):\n",
    "            decoded_words.append(\"<EOS>\")\n",
    "            break\n",
    "        elif (ni != 0):\n",
    "            #print(ni)\n",
    "            decoded_words.append(index2word.get(ni,0))\n",
    "\n",
    "        next_input = w2v_embeddings[ni]\n",
    "        decoder_input = Variable(torch.FloatTensor(next_input).unsqueeze(0).unsqueeze(0)) \n",
    "        decoder_input = decoder_input.cuda() \n",
    "        \n",
    "    s = \" \"\n",
    "    predicted = decoded_words\n",
    "    #print(s.join(sentence))\n",
    "    #print(s.join(predicted))\n",
    "    bleu_score = bleu(sentence,predicted)\n",
    "    return bleu_score,hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer on validation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> a man and woman at a table with beer and wine <EOS>\n",
      "<SOS> a man and woman at a table with beer and wine\n",
      "<SOS> a man speaking into a microphone on a stage with a bicycle and dressed in cyclist gear . <EOS>\n",
      "<SOS> a man into a a with a bike and dressed in gear .\n",
      "<SOS> four horses are skattered around a small water hole . <EOS>\n",
      "<SOS> three are around a small\n",
      "<SOS> a man and a young girl playing wii <EOS>\n",
      "<SOS> a man and a young girl playing wii\n",
      "<SOS> a boat home sitting on a river bay . <EOS>\n",
      "<SOS> a boat home sitting a\n",
      "<SOS> several tim 's of mints are stacked up with a bottle that has several clipped roses inside <EOS>\n",
      "<SOS> several 's of are stacked up with a bottle that has three inside\n",
      "<SOS> family at a pizza restaurant posing for a picture before meal . <EOS>\n",
      "<SOS> family at a pizza restaurant posing for a picture before meal\n",
      "<SOS> several mopeds are lined up along the side of a hotel parking lot . <EOS>\n",
      "<SOS> several are lined up along the side of a parking lot\n",
      "<SOS> a young man appears to be taking a break from the waves . <EOS>\n",
      "<SOS> a young man to be taking a from the waves\n",
      "<SOS> a baseball player standing next to home plate with a bat . <EOS>\n",
      "<SOS> a baseball player standing next to home tray with a\n",
      "<SOS> a man sitting on a motorcycle in an empty parking lot <EOS>\n",
      "<SOS> a man sitting a in an empty parking lot\n",
      "<SOS> a girl rides her skateboard in a public place <EOS>\n",
      "<SOS> a girl rides her skateboard in a public place .\n",
      "<SOS> two men gesture hands next to laptops , one man uses a phone . <EOS>\n",
      "<SOS> two men hands next to laptops , one man a phone\n",
      "<SOS> a couple of women sitting at a hair salon . <EOS>\n",
      "<SOS> a couple of women sitting at a hair\n",
      "<SOS> a furnished neutral modern open floor plan . <EOS>\n",
      "<SOS> a modern open floor\n",
      "<SOS> a man standing near a table with video equipment . <EOS>\n",
      "<SOS> a man standing near a table with video equipment\n",
      "<SOS> a close-up picture of some food on paper plates <EOS>\n",
      "<SOS> a picture of some food and\n",
      "<SOS> a male baseball player wearing red and white is up to bat . <EOS>\n",
      "<SOS> a male baseball player wearing red and white up to\n",
      "<SOS> the propeller of a white plane flying and a river <EOS>\n",
      "<SOS> the of a yellow flying and a\n",
      "<SOS> two birds standing on an floating on a body of water . <EOS>\n",
      "<SOS> two birds standing an floating a body of\n",
      "<SOS> people are riding on skis in the snow on a street . <EOS>\n",
      "<SOS> people are riding skis in the snow a street\n",
      "<SOS> a pot of vegetable soup is cooking on the stove . <EOS>\n",
      "<SOS> a pot of things is cooking the stove\n",
      "<SOS> a part of a silver utensil on a gray surface . <EOS>\n",
      "<SOS> a of a silver a gray surface .\n",
      "<SOS> a hot dog in a napkin with onions and green peppers <EOS>\n",
      "<SOS> a hot dog in a with tomato and green peppers .\n",
      "<SOS> a young boy holding an umbrella while standing on a wooden deck . <EOS>\n",
      "<SOS> a young boy holding an umbrella while standing a wooden\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "hidden_states = []\n",
    "softmax = nn.Softmax()\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "encoder.load_state_dict(torch.load('./encoderW.pth'))\n",
    "\n",
    "decoder.load_state_dict(torch.load('./decoderW2.pth'))\n",
    "\n",
    "Total_bleu = 0 \n",
    "for sentence in sentences[:25]:\n",
    "    \n",
    "    blue_score,hidden_states = seq2seq_inference(sentence,encoder,decoder,hidden_states)\n",
    "    Total_bleu += blue_score\n",
    "\n",
    "\n",
    "#np.save(open('outputs/hidden_states_val_W', 'wb+'), hidden_states)     \n",
    "#print(\"Average bleu score:\",Total_bleu/500)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is making better predictions on validation data compared to last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bleu score: 0.4768919166049488\n"
     ]
    }
   ],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "hidden_states = []\n",
    "softmax = nn.Softmax()\n",
    "index2word = {index:word for index,word in enumerate(vocabulary)}\n",
    "\n",
    "encoder.load_state_dict(torch.load('./encoderW.pth'))\n",
    "\n",
    "decoder.load_state_dict(torch.load('./decoderW2.pth'))\n",
    "\n",
    "Total_bleu = 0 \n",
    "for sentence in sentences[:500]:\n",
    "    \n",
    "    blue_score,hidden_states = seq2seq_inference(sentence,encoder,decoder,hidden_states)\n",
    "    Total_bleu += blue_score\n",
    "\n",
    "\n",
    "np.save(open('outputs/hidden_states_val_W', 'wb+'), hidden_states)     \n",
    "print(\"Average bleu score:\",Total_bleu/500)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bleu score is higher when compared to the one hot representation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man and woman at a table with beer and wine\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "A man speaking into a microphone on a stage with a bicycle and dressed in cyclist gear.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "Four horses are skattered around a small water hole.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "A man and a young girl playing Wii\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "A boat home sitting on a river bay.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "Several Tim's of mints are stacked up with a bottle that has several  clipped roses inside\n",
      "United Postal Service truck drives over very snowy roads\n",
      "\n",
      "Family at a pizza restaurant posing for a picture before meal.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "Several mopeds are lined up along the side of a hotel parking lot.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "A young man appears to be taking a break from the waves.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n",
      "A baseball player standing next to home plate with a bat.\n",
      "5 adults strangely dressed about to snow ski\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now get nearest neighbors and print\n",
    "import math\n",
    "\n",
    "epoch = 0\n",
    "f_train = np.load(open('outputs/hidden_states_train_W'+str(epoch), 'rb'))\n",
    "f_val = np.load(open('outputs/hidden_states_val_W', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "for val_id in range(10):\n",
    "       \n",
    "    val_vec = f_val[val_id]\n",
    "    \n",
    "    min_id = 0\n",
    "    min_dist = math.inf\n",
    "    for train_id in range(400000):\n",
    "        train_vec = f_train[train_id]\n",
    "        \n",
    "        dist = np.square(np.linalg.norm(val_vec-train_vec))\n",
    "        \n",
    "        if (dist < min_dist):\n",
    "            min_dist = dist\n",
    "            min_id = train_id\n",
    "            \n",
    "      \n",
    "    #print(min_id)\n",
    "    print(val_sentences[val_id])\n",
    "    print(train_sentences[min_id])\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot explain why this is happening. Even though the prediction performance is good, the nearest representation always gives only 2 sentences for all validation sentences (I tried for most). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# With Mini Batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is not complete. I have not made changes to the custom RNN model to incorporate pad packed sequences. I have got the tensors ready but it appears that to run pad packed sequences with custom RNN models we have to make changes to the way we describe the model, which is not done here. Hence, the compile error.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<pad>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def numberize(sentence):\n",
    "    numberized = [word2index.get(word, 0) for word in sentence]\n",
    "    return numberized\n",
    "\n",
    "def one_hot(sentence):\n",
    "    numberized = numberize(sentence)\n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.cuda.LongTensor of size 1x30x300 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.addmm received an invalid combination of arguments - got (int, torch.cuda.LongTensor, int, torch.cuda.LongTensor, torch.cuda.FloatTensor, out=torch.cuda.LongTensor), but expected one of:\n * (torch.cuda.LongTensor source, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, int alpha, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, int alpha, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, int alpha, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.LongTensor, int, torch.cuda.LongTensor, !torch.cuda.FloatTensor!, out=torch.cuda.LongTensor)\n * (int beta, torch.cuda.LongTensor source, int alpha, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.LongTensor, int, !torch.cuda.LongTensor!, !torch.cuda.FloatTensor!, out=torch.cuda.LongTensor)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e9b7f3bbd11a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./encoderBW.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e9b7f3bbd11a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#print(packed_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;31m#encoder_outputs[ei] = encoder_output[0][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0f99e3101de7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden, state)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0migates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mhgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfusedBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMFused\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(tensor1, tensor2, out)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim_tensor1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim_tensor2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAddmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[0;32m---> 26\u001b[0;31m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.LongTensor, int, torch.cuda.LongTensor, torch.cuda.FloatTensor, out=torch.cuda.LongTensor), but expected one of:\n * (torch.cuda.LongTensor source, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, int alpha, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (torch.cuda.LongTensor source, int alpha, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n * (int beta, torch.cuda.LongTensor source, int alpha, torch.cuda.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.LongTensor, int, torch.cuda.LongTensor, !torch.cuda.FloatTensor!, out=torch.cuda.LongTensor)\n * (int beta, torch.cuda.LongTensor source, int alpha, torch.cuda.sparse.LongTensor mat1, torch.cuda.LongTensor mat2, *, torch.cuda.LongTensor out)\n      didn't match because some of the arguments have invalid types: (int, torch.cuda.LongTensor, int, !torch.cuda.LongTensor!, !torch.cuda.FloatTensor!, out=torch.cuda.LongTensor)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, epochs, learning_rate):\n",
    "    \n",
    "    hidden_states = []\n",
    "    plot_loss_total = 0  \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        count = 0\n",
    "        for bch in range(2):\n",
    "            \n",
    "            vectorized_sen = [[one_hot(sen)]for sen in sentences[(bch*batch_size)+1:(bch+1)*batch_size]]\n",
    "\n",
    "            seq_lengths = []\n",
    "            for i in range(len(vectorized_sen)):\n",
    "                seq_lengths.append(len(vectorized_sen[i][0]))\n",
    "    \n",
    "\n",
    "            seq_tensor = Variable(torch.zeros((len(vectorized_sen),maxSequenceLength,1000))).long().cuda()\n",
    "            for idx, (seq, seqlen) in enumerate(zip(vectorized_sen, seq_lengths)):\n",
    "                seq_tensor[idx,:seqlen,:1000] = torch.FloatTensor(seq)\n",
    "    \n",
    "            seq_lengths = torch.cuda.LongTensor(seq_lengths)\n",
    "            seq_tensor = seq_tensor.long()\n",
    "            # SORT YOUR TENSORS BY LENGTH!\n",
    "            seq_lengths, perm_idx = seq_lengths.sort(0,descending=True)\n",
    "            seq_tensor = seq_tensor[perm_idx]\n",
    "            seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "\n",
    "            #pack them up \n",
    "            packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy())\n",
    "\n",
    "            encoder_hidden,encoder_state = encoder.initHidden()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_outputs = Variable(torch.zeros(maxSequenceLength,batch_size, hidden_size))\n",
    "            encoder_outputs = encoder_outputs.cuda() \n",
    "\n",
    "            #ERROR HERE     \n",
    "            encoder_output, encoder_hidden,encoder_state = encoder(packed_input,encoder_hidden,encoder_state)\n",
    "            \n",
    "    \n",
    "                                 \n",
    "            decoder_input = packed_input\n",
    "            hidden_states.append(encoder_hidden.squeeze().cpu().data.numpy()) \n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "                    \n",
    "            decoder_output,decoder_hidden,decoder_state = decoder(decoder_input,decoder_hidden,\n",
    "                                                                               encoder_output)\n",
    "            \n",
    "            #change this \n",
    "            '''         \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            ''' \n",
    "                    \n",
    "            loss += criterion(decoder_output.squeeze(0), label)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "                        \n",
    "            plot_loss_total += loss.data[0] / nWords\n",
    "            \n",
    "            if (count % 50 == 0):\n",
    "                print(plot_loss_total/50)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "        \n",
    "        np.save(open('outputs/hidden_states_train_BW'+str(epoch), 'wb+'), hidden_states)    \n",
    "           \n",
    "\n",
    "        \n",
    "epochs = 1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "trainIters(encoder,decoder,epochs,learning_rate)\n",
    "\n",
    "torch.save(encoder.state_dict(), './encoderBW.pth')\n",
    "torch.save(decoder.state_dict(), './decoderBW.pth')\n",
    "print('training done')   \n",
    "\n",
    "\n",
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output,ht,ct = decoder(packed_input,ht,ct)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
