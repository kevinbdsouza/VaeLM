{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "#from tensorflow.python.ops.functional_ops import map_fn as map_fn\n",
    "map_fn = tf.map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/kevindsouza/Documents/UBC/Term2/CPSC532L/Project/TensorFlow/graphs-and-NLP/LSTM/LSTM/simple-examples/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename,\"r\") as f:\n",
    "        return f.read().replace(\"\\n\",\"<eos>\").split()\n",
    "    \n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "    \n",
    "    #print(data[1])\n",
    "    counter = collections.Counter(data)\n",
    "    #print(counter)\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: (-x[1],x[0]))\n",
    "    #print(count_pairs)\n",
    "    \n",
    "    words,_ = list(zip(*count_pairs))\n",
    "    \n",
    "    word_to_id = dict(zip(words,range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(filename,word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def load_data():\n",
    "    train_path = os.path.join(data_path,\"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path,\"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path,\"ptb.test.txt\")\n",
    "    \n",
    "    #build vocab and then convert into list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path,word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path,word_to_id)\n",
    "    test_data = file_to_word_ids(test_path,word_to_id)\n",
    "    \n",
    "    vocabulary = len(word_to_id)\n",
    "    \n",
    "    reversed_dictionary = dict(zip(word_to_id.values(),word_to_id.keys()))\n",
    "    \n",
    "    print(train_data[:5])\n",
    "    print(vocabulary)\n",
    "    #print(word_to_id)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    \n",
    "    return train_data,valid_data,test_data,vocabulary,reversed_dictionary \n",
    "\n",
    "\n",
    "train_data,valid_data,test_data,vocabulary,reversed_dictionary = load_data()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_producer(raw_data,batch_size,num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data,name=\"raw_data\",dtype=tf.int32)\n",
    "    \n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    \n",
    "    data = tf.reshape(raw_data[0:batch_size*batch_len],[batch_size,batch_len])\n",
    "    \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    #print(i)\n",
    "    x = data[:,i*(num_steps):(i+1)*num_steps]\n",
    "    #print(x)\n",
    "    x.set_shape([batch_size,num_steps])\n",
    "    y = data[:,i*(num_steps) + 1:(i+1)*num_steps + 1]\n",
    "    y.set_shape([batch_size,num_steps])\n",
    "    \n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE    = 2       # 2 bits per timestep\n",
    "RNN_HIDDEN    = 20\n",
    "OUTPUT_SIZE   = 1       # 1 bit per timestep\n",
    "TINY          = 1e-6    # to avoid NaNs in logs\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "USE_LSTM = True\n",
    "\n",
    "inputs  = tf.placeholder(tf.float32, (None, None, INPUT_SIZE))  # (time, batch, in)\n",
    "outputs = tf.placeholder(tf.float32, (None, None, OUTPUT_SIZE)) # (time, batch, out)\n",
    "\n",
    "\n",
    "## Here cell can be any function you want, provided it has two attributes:\n",
    "#     - cell.zero_state(batch_size, dtype)- tensor which is an initial value\n",
    "#                                           for state in __call__\n",
    "#     - cell.__call__(input, state) - function that given input and previous\n",
    "#                                     state returns tuple (output, state) where\n",
    "#                                     state is the state passed to the next\n",
    "#                                     timestep and output is the tensor used\n",
    "#                                     for infering the output at timestep. For\n",
    "#                                     example for LSTM, output is just hidden,\n",
    "#                                     but state is memory + hidden\n",
    "# Example LSTM cell with learnable zero_state can be found here:\n",
    "#    https://gist.github.com/nivwusquorum/160d5cf7e1e82c21fad3ebf04f039317\n",
    "if USE_LSTM:\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(RNN_HIDDEN, state_is_tuple=True)\n",
    "else:\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(RNN_HIDDEN)\n",
    "\n",
    "# Create initial state. Here it is just a constant tensor filled with zeros,\n",
    "# but in principle it could be a learnable parameter. This is a bit tricky\n",
    "# to do for LSTM's tuple state, but can be achieved by creating two vector\n",
    "# Variables, which are then tiled along batch dimension and grouped into tuple.\n",
    "batch_size    = tf.shape(inputs)[1]\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "print(initial_state)\n",
    "\n",
    "# Given inputs (time, batch, input_size) outputs a tuple\n",
    "#  - outputs: (time, batch, output_size)  [do not mistake with OUTPUT_SIZE]\n",
    "#  - states:  (time, batch, hidden_size)\n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, time_major=True)\n",
    "\n",
    "# project output from rnn output size to OUTPUT_SIZE. Sometimes it is worth adding\n",
    "# an extra layer here.\n",
    "final_projection = lambda x: layers.linear(x, num_outputs=OUTPUT_SIZE, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "# apply projection to every timestep.\n",
    "predicted_outputs = map_fn(final_projection, rnn_outputs)\n",
    "\n",
    "# compute elementwise cross entropy.\n",
    "error = -(outputs * tf.log(predicted_outputs + TINY) + (1.0 - outputs) * tf.log(1.0 - predicted_outputs + TINY))\n",
    "error = tf.reduce_mean(error)\n",
    "\n",
    "# optimize\n",
    "train_fn = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(error)\n",
    "\n",
    "# assuming that absolute difference between output and correct answer is 0.5\n",
    "# or less we can round it to the correct output.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.abs(outputs - predicted_outputs) < 0.5, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 35\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "valid_x, valid_y = batch_producer(train_data,BATCH_SIZE,NUM_STEPS)\n",
    "\n",
    "session = tf.Session()\n",
    "# For some reason it is our job to do this:\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_error = 0\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        # here train_fn is what triggers backprop. error and accuracy on their\n",
    "        # own do not trigger the backprop.\n",
    "        x, y = generate_batch(num_bits=NUM_BITS, batch_size=BATCH_SIZE)\n",
    "        epoch_error += session.run([error, train_fn], {\n",
    "            inputs: x,\n",
    "            outputs: y,\n",
    "        })[0]\n",
    "    epoch_error /= ITERATIONS_PER_EPOCH\n",
    "    valid_accuracy = session.run(accuracy, {\n",
    "        inputs:  valid_x,\n",
    "        outputs: valid_y,\n",
    "    })\n",
    "    print(\"Epoch {:d}, train error {:.2f}, valid accuracy {:.1f}\".format(epoch, epoch_error, valid_accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
