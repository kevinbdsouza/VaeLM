{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/kevindsouza/Documents/UBC/Term2/CPSC532L/Project/TensorFlow/graphs-and-NLP/LSTM/LSTM/simple-examples/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('run_opt',type=int, default=2, help='A integer: 1 to train, 2 to test')\n",
    "#parser.add_argument('--data_path',type=str,default=data_path,help='the full path of training data')\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9970, 9971, 9972, 9974, 9975]\n",
      "10000\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec\n"
     ]
    }
   ],
   "source": [
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename,\"r\") as f:\n",
    "        return f.read().replace(\"\\n\",\"<eos>\").split()\n",
    "    \n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "    \n",
    "    #print(data[1])\n",
    "    counter = collections.Counter(data)\n",
    "    #print(counter)\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: (-x[1],x[0]))\n",
    "    #print(count_pairs)\n",
    "    \n",
    "    words,_ = list(zip(*count_pairs))\n",
    "    \n",
    "    word_to_id = dict(zip(words,range(len(words))))\n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(filename,word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def load_data():\n",
    "    train_path = os.path.join(data_path,\"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path,\"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path,\"ptb.test.txt\")\n",
    "    \n",
    "    #build vocab and then convert into list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path,word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path,word_to_id)\n",
    "    test_data = file_to_word_ids(test_path,word_to_id)\n",
    "    \n",
    "    vocabulary = len(word_to_id)\n",
    "    \n",
    "    reversed_dictionary = dict(zip(word_to_id.values(),word_to_id.keys()))\n",
    "    \n",
    "    print(train_data[:5])\n",
    "    print(vocabulary)\n",
    "    #print(word_to_id)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    \n",
    "    return train_data,valid_data,test_data,vocabulary,reversed_dictionary \n",
    "\n",
    "\n",
    "train_data,valid_data,test_data,vocabulary,reversed_dictionary = load_data()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_producer(raw_data,batch_size,num_steps):\n",
    "    raw_data = tf.convert_to_tensor(raw_data,name=\"raw_data\",dtype=tf.int32)\n",
    "    \n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    \n",
    "    data = tf.reshape(raw_data[0:batch_size*batch_len],[batch_size,batch_len])\n",
    "    \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    #print(i)\n",
    "    x = data[:,i*(num_steps):(i+1)*num_steps]\n",
    "    #print(x)\n",
    "    x.set_shape([batch_size,num_steps])\n",
    "    y = data[:,i*(num_steps) + 1:(i+1)*num_steps + 1]\n",
    "    y.set_shape([batch_size,num_steps])\n",
    "    \n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(object):\n",
    "    def __init__(self,batch_size,num_steps,data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data)//batch_size)-1) // num_steps\n",
    "        self.input_data,self.targets = batch_producer(data,batch_size,num_steps)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self,input,is_training,hidden_size,vocab_size,num_layers,dropout=0.5,init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input \n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps \n",
    "        self.hidden_size = hidden_size \n",
    "        \n",
    "        \n",
    "        #create the word embeddings \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size,self.hidden_size],-init_scale,init_scale))\n",
    "            inputs = tf.nn.embedding_lookup(embedding,self.input_obj.input_data)\n",
    "        \n",
    "        #print(inputs)\n",
    "        \n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs,dropout)\n",
    "            \n",
    "            \n",
    "        \n",
    "        #set up state storage and extraction\n",
    "        self.init_state = tf.placeholder(tf.float32,[num_layers,2,self.batch_size,self.hidden_size])\n",
    "        \n",
    "        #print(self.init_state)\n",
    "        state_per_layer_list = tf.unstack(self.init_state,axis=0)\n",
    "        rnn_tuple_state = [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0],\n",
    "                                 state_per_layer_list[idx][1]) for idx in range(num_layers)]\n",
    "        \n",
    "        #print(inputs)\n",
    "        print(rnn_tuple_state)\n",
    "        #create LSTM cell to be unrolled \n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size,forget_bias=1.0,state_is_tuple=True)\n",
    "        \n",
    "        if is_training and dropout < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob = dropout)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple = True) \n",
    "            \n",
    "        output,self.state = tf.nn.dynamic_rnn(cell,inputs,initial_state = rnn_tuple_state, dtype=tf.float32)\n",
    "                        \n",
    "        \n",
    "        output = tf.reshape(output,[-1,hidden_size])\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.random_uniform[hidden_size,vocab_size],-init_scale,init_scale)\n",
    "        softmax_b = tf.Variable(tf.random_uniform[vocab_size],-init_scale,init_scale)\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output,softax_w,softmax_b)\n",
    "        \n",
    "        #reshape logits to be a 3D tensor for sequence loss\n",
    "        logits = tf.reshape(logits,[self.batch_size,self.num_steps,vocab_size])\n",
    "        \n",
    "        #use the contrib sequence loss and average over the batches \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits,self.input_obj.targets,\n",
    "                                               tf.ones([self.batch_size,self.num_steps],dtype=np.float32),\n",
    "                                                average_across_timesteps = False,\n",
    "                                                average_across_batch = True)\n",
    "        #update cost \n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "        \n",
    "        #get prediction accuracy\n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits,[-1,vocab_size]))\n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out,axis=1),tf.int32)\n",
    "        correct_prediction = tf.equal(self.predict,tf.reshape(self.input_obj.targets,[-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        if not is_training:\n",
    "            return \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.learning_rate = tf.Variable(0.0,trainable = False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads,_ = tf.clip_by_global_norms(tf.gradients(self.cost,tvars),5)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        \n",
    "        self.train_op = optimizer.applygradients(zip(grads,tvars), \n",
    "                                                 global_step = tf.contrib.framework.get_or_create_global_step())\n",
    "        \n",
    "        self.new_lr = tf.placeholder(tf.floa32,[])\n",
    "        self.lr_update = tf.assign(self.learning_rate,self.new_lr)\n",
    "        \n",
    "        \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        \n",
    "        session.run(self.lr_update, feed_dict={self.new_lr:lr_value})\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,vocabulary,num_layers,num_epochs,batch_size,model_save_name,learning_rate=1.0,\n",
    "          max_lr_epoch=10,lr_decay=0.93,print_iter=50):\n",
    "    \n",
    "    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n",
    "    print(training_input.input_data)\n",
    "    print(training_input.targets)\n",
    "    \n",
    "    m = Model(training_input,is_training = True,hidden_size=650,vocab_size=vocabulary,\n",
    "             num_layers=num_layers)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    orig_decay = lr_decay\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run([init_op])\n",
    "        coord = tf.train.Coordinator() \n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch+1-max_lr_epoch,0.0)        \n",
    "            m.assign_lr(sess,learning_rate*new_lr_decay)\n",
    "            \n",
    "            current_state = np.zeros((num_layers,2,batch_size,m.hidden_size))\n",
    "            curr_time = dt.datetime.now()\n",
    "            \n",
    "            for step in range(training_input.epoch_size):\n",
    "                if step % print_iter != 0:\n",
    "                    cost,_,current_state = sess.run([m.cost,m.train_op,m.state],\n",
    "                                                    feed_dict={m.init_state:current_state})\n",
    "                    \n",
    "                else:\n",
    "                    seconds = (float((dt.datetime.now() - curr_time).seconds) // print_iter)\n",
    "                    curr_time = dt.datetime.now()\n",
    "                    cost,_,current_state,acc = sess.run([m.cost,m.train,m.state,m.accuracy],\n",
    "                                                       feed_dict={m.init_state:current_state})\n",
    "                    print(\"Epoch {}, step {}, cost {:.3f},accuracy {:,3f},seconds_per_step {:.3f}\".format(\n",
    "                    epoch,step,cost,acc,seconds))\n",
    "                    \n",
    "            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n",
    "            \n",
    "        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n",
    "        \n",
    "        #close threads\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_1:0\", shape=(50, 35), dtype=int32)\n",
      "Tensor(\"strided_slice_2:0\", shape=(50, 35), dtype=int32)\n",
      "Tensor(\"embedding_lookup:0\", shape=(50, 35, 650), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"dropout/mul:0\", shape=(50, 35, 650), dtype=float32)\n",
      "[LSTMStateTuple(c=<tf.Tensor 'strided_slice_3:0' shape=(50, 650) dtype=float32>, h=<tf.Tensor 'strided_slice_4:0' shape=(50, 650) dtype=float32>)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2caf42fba2cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lstm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_save_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-4b5943ba4b7e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name, learning_rate, max_lr_epoch, lr_decay, print_iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     m = Model(training_input,is_training = True,hidden_size=650,vocab_size=vocabulary,\n\u001b[0;32m----> 9\u001b[0;31m              num_layers=num_layers)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5279699011a6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, is_training, hidden_size, vocab_size, num_layers, dropout, init_scale)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_tuple_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    818\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[1;32m   2932\u001b[0m         swap_memory=swap_memory)\n\u001b[1;32m   2933\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2935\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2718\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2720\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2721\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2660\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2661\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2911\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2912\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2913\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    793\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    794\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recurrent_input_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                              self._input_keep_prob)\n\u001b[0;32m-> 1042\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_should_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_keep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m       \u001b[0;31m# Identify which subsets of the state to perform dropout on and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 292\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowEnv/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0mc_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m       \u001b[0mc_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "num_epochs = 2\n",
    "batch_size = 50\n",
    "model_save_name = \"lstm\"\n",
    "\n",
    "train(train_data,vocabulary,num_layers,num_epochs,batch_size,model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
