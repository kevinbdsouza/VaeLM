{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mode):\n",
    "\n",
    "    #load text files \n",
    "    train_sentences = [line.strip() for line in open(\"data/english/train.txt\").readlines()]\n",
    "    val_sentences = [line.strip() for line in open(\"data/english/valid.txt\").readlines()]\n",
    "    test_sentences = [line.strip() for line in open(\"data/english/test.txt\").readlines()]\n",
    "    train_sentences = [x for x in train_sentences if x] \n",
    "    val_sentences = [x for x in val_sentences if x] \n",
    "    test_sentences = [x for x in test_sentences if x]\n",
    "\n",
    "    if mode == \"train\":\n",
    "        sentences = train_sentences\n",
    "        max_char_len = 494\n",
    "    elif mode == \"val\":\n",
    "        sentences = val_sentences\n",
    "        max_char_len = 356\n",
    "    elif mode == \"test\":\n",
    "        sentences = test_sentences\n",
    "        max_char_len = 463\n",
    "    sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "    #set > as unk \n",
    "    for ind,sen in enumerate(sentences):\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                sen.remove(\"<\")\n",
    "                sen.remove(\"unk\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    #define vocab \n",
    "    vocabulary = [\"<SOS>\"] + [\"a\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"f\"] + \\\n",
    "    [\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"o\"] + \\\n",
    "    [\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + \\\n",
    "    [\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "    [\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "    [\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] \n",
    "\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "    index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "    one_hot_embeddings = np.eye(vocabulary_size)\n",
    "\n",
    "    #find max word length \n",
    "    max_word_length = 0\n",
    "    maxid = 0\n",
    "    for i in range(len(sentences)):\n",
    "        l = len(sentences[i])\n",
    "        if l > max_word_length:\n",
    "            maxid = i\n",
    "            max_word_length = l\n",
    "\n",
    "\n",
    "\n",
    "    return sentences,vocabulary_size,max_word_length,one_hot_embeddings,token2index,max_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce character embeddings \n",
    "def embed_producer(sentences,vocabulary_size,max_word_length,one_hot_embeddings,token2index,max_char_len):\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    eow_loc_all = np.zeros((len(sentences),max_char_len))\n",
    "    sen_lens = []\n",
    "    num_words = []\n",
    "    max_char = 0\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        eow_loc = np.zeros(max_char_len)\n",
    "        prev = 0\n",
    "        count = 0 \n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    count+=1\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            elif (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                count +=1\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1 \n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        eow_loc_all[i,:] = eow_loc\n",
    "        n_w = int(np.sum(eow_loc_all[i]))\n",
    "        \n",
    "        num_words.append(2*n_w - 1)\n",
    "        sen_lens.append(count+1)\n",
    "        \n",
    "        if (count+1 > max_char):\n",
    "            max_char = count+1 \n",
    "            \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,eow_loc_all,sen_lens,num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocess(mode):\n",
    "\t#preprocess the data \n",
    "\tsentences,vocabulary_size,max_word_length,one_hot_embeddings,token2index,max_char_len = preprocess(mode)\n",
    "\t#produce embeddings \n",
    "\tdata,eow_loc_all,sen_lens,num_words = embed_producer(sentences,vocabulary_size,max_word_length,one_hot_embeddings,token2index,max_char_len)\n",
    "\n",
    "\treturn data,eow_loc_all,sen_lens,num_words,vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/PTB/ptb.train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7b90e9969add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meow_loc_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msen_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3bcaf71f1c93>\u001b[0m in \u001b[0;36mrun_preprocess\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m#preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_word_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken2index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_char_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m#produce embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meow_loc_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msen_lens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_producer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_word_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken2index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_char_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3d37bc6a9b78>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#load text files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/PTB/ptb.train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mval_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/PTB/ptb.valid.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/PTB/ptb.test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/PTB/ptb.train.txt'"
     ]
    }
   ],
   "source": [
    "mode = \"val\"\n",
    "data,eow_loc_all,sen_lens,num_words,vocabulary_size = run_preprocess(mode)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "\tdef __init__(self,**kwargs):\n",
    "\t\t#self.data =kwargs['data']\n",
    "\t\t#self.sentences =kwargs['sentences']\n",
    "\t\t#self.vocabulary_size = kwargs['vocabulary_size']\n",
    "\t\t#self.max_word_length = kwargs['max_word_length']\n",
    "\t\tself.max_char_len = kwargs['max_char_len']\n",
    "\t\tself.batch_size=kwargs['batch_size']\n",
    "\t\tself.input_size=kwargs['input_size']\n",
    "\t\tself.hidden_size =kwargs['hidden_size']\n",
    "\n",
    "\n",
    "\t# our [494, 52, 61] tensor becomes [[52, 61], [52, 61], ...]\n",
    "\tdef run_encoder(self,inputs,word_pos,reuse):\n",
    "\n",
    "\t\tinputs_t = tf.transpose(inputs,perm=[1, 0, 2])\n",
    "\t\t_inputs_ta = tf.TensorArray(dtype=tf.float32, size=self.max_char_len,name='char_array')\n",
    "\t\t_inputs_ta = _inputs_ta.unstack(inputs_t) \n",
    "\n",
    "\t\tcell = tf.contrib.rnn.LSTMCell(self.hidden_size)\n",
    "\t\toutput_ta = tf.TensorArray(size=self.max_char_len, dtype=tf.float32,name='word_array')\n",
    "\t\tmean_ta = tf.TensorArray(size=self.max_char_len, dtype=tf.float32,name='mean_array')\n",
    "\t\tsigma_ta = tf.TensorArray(size=self.max_char_len, dtype=tf.float32,name='sigma_array')\n",
    "\t\tword_pos = tf.convert_to_tensor(word_pos,dtype=tf.float32)\n",
    "\n",
    "\t\t# create loop_fn for raw_rnn\n",
    "\t\tdef loop_fn(time, cell_output, cell_state, loop_state):\n",
    "\t\t    emit_output = cell_output  # == None if time = 0\n",
    "\n",
    "\t\t    if cell_output is None:  # time = 0\n",
    "\t\t        next_cell_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "\t\t        sample_loop_state = output_ta\n",
    "\t\t        mean_loop_state = mean_ta\n",
    "\t\t        sigma_loop_state = sigma_ta\n",
    "\t\t        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\n",
    "\t\t    else:\n",
    "\t\t        word_slice = tf.tile(word_pos[:,time-1],[20])\n",
    "\t\t        word_slice = tf.reshape(word_slice,[20,52])\n",
    "\t\t        word_slice = tf.transpose(word_slice,perm=[1,0])\n",
    "\t\t        next_sampled_input =  tf.multiply(cell_output,word_slice)\n",
    "\t\t        \n",
    "\t\t        #reparametrization\n",
    "\t\t        z_concat = tf.contrib.layers.fully_connected(next_sampled_input,2*self.hidden_size)\n",
    "\t\t        z_mean = z_concat[:,:20]\n",
    "\t\t        z_log_sigma_sq =  z_concat[:,20:40]\n",
    "\t\t        eps = tf.random_normal((self.batch_size,self.hidden_size),0,1,dtype=tf.float32)\n",
    "\t\t        z_sample = tf.add(z_mean,tf.multiply(tf.sqrt(tf.exp(z_log_sigma_sq)),eps))\n",
    "\t\t        \n",
    "\t\t        z_sample = tf.multiply(z_sample,word_slice)\n",
    "\t\t        z_mean = tf.multiply(z_mean,word_slice)\n",
    "\t\t        z_log_sigma_sq = tf.multiply(z_log_sigma_sq,word_slice)\n",
    "\t\t        \n",
    "\t\t        next_cell_state = z_sample\n",
    "\t\t        sample_loop_state = loop_state[0].write(time - 1, next_cell_state)\n",
    "\t\t        mean_loop_state = loop_state[1].write(time - 1, z_mean)\n",
    "\t\t        sigma_loop_state = loop_state[2].write(time - 1, z_log_sigma_sq)\n",
    "\t\t        next_loop_state = (sample_loop_state,mean_loop_state,sigma_loop_state)\n",
    "\t\t        \n",
    "\t\t        word_slice = tf.logical_not(tf.cast(word_slice,dtype=tf.bool))\n",
    "\t\t        word_slice = tf.cast(word_slice,dtype=tf.float32)\n",
    "\t\t        next_cell_state = next_cell_state + tf.multiply(cell_state[0],word_slice)\n",
    "\t\t        next_cell_state = tf.contrib.rnn.LSTMStateTuple(next_cell_state,cell_output)\n",
    "\n",
    "\t\t    elements_finished = (time >= (self.max_char_len)-1)\n",
    "\t\t    next_input = _inputs_ta.read(time)\n",
    "\n",
    "\t\t    return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "\t\twith tf.variable_scope('encoder_rnn',reuse=reuse):\n",
    "\t\t\toutputs_ta, final_state_out, word_state = tf.nn.raw_rnn(cell, loop_fn)\n",
    "\t\t\t\n",
    "\t\tword_state_out = word_state[0].stack()\n",
    "\t\tmean_state_out = word_state[1].stack()\n",
    "\t\tsigma_state_out = word_state[2].stack()\n",
    "\t\toutputs_out = outputs_ta.stack()\n",
    "\n",
    "\t\treturn word_state_out,mean_state_out,sigma_state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "num_batches = len(data) // batch_size\n",
    "input_size = vocabulary_size\n",
    "batch_size = 52\n",
    "max_char_len = 494\n",
    "hidden_size   = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([init_op])\n",
    "    for epoch in range(1):\n",
    "        epoch_error = 0\n",
    "        \n",
    "        for bt in range(3):\n",
    "            x = data[bt*batch_size:(bt+1)*batch_size]\n",
    "            word_pos_batch = eow_loc_all[bt*batch_size:(bt+1)*batch_size]\n",
    "            outputs,final_state,word_state,mean_state,sigma_state = sess.run([outputs_out, final_state_out, \n",
    "                                                                        word_state_out,mean_state_out,sigma_state_out],\n",
    "                                                                        feed_dict={inputs:x,word_pos:word_pos_batch})\n",
    "                                                                   \n",
    "'''                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.13253187  0.          0.\n",
      "  0.          0.17314994  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.03053938  0.\n",
      "  0.14000218  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.00622426  0.\n",
      "  0.12139752  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.10800748  0.\n",
      "  0.39081505  0.          0.15581407  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.03140066  0.          0.1720185\n",
      "  0.          0.2231635   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.20605411  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.10539071  0.          0.23587176  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.         0.         0.         0.         0.0951355  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "[ 0.          0.          0.          0.          0.23719308  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.28043571  0.          0.\n",
      "  0.          0.13979147  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.0458578  0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.07615764  0.          0.05308381  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.        0.        0.        0.        0.118598  0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.      ]\n",
      "[ 0.          0.          0.          0.          0.18050811  0.          0.\n",
      "  0.          0.01390371  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.19290465  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.16620295  0.\n",
      "  0.00801427  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.25603911  0.\n",
      "  0.00755719  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.00078096  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.07006556  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "#print(word_state.shape)\n",
    "#print(mean_state.shape)\n",
    "#print(sigma_state.shape)\n",
    "#word_state = word_state[1]\n",
    "#print(word_state.shape)\n",
    "for i in range(20):\n",
    "    print(word_state[6,:,i])\n",
    "    #print(mean_state[2,:,i])\n",
    "    #print(sigma_state[3,:,i])\n",
    "    \n",
    "#print(sigma_state[1][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
