{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "\n",
    "from collections import Counter\n",
    "from random import random\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3003\n"
     ]
    }
   ],
   "source": [
    "mode = \"valid\"\n",
    "lang = \"english\"\n",
    "sentences = [line.strip() for line in open(\"data/\"+lang+\"/\"+mode+\".txt\").readlines()]\n",
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33997\n",
      "3003\n",
      "3003\n",
      "reprise de la session\n",
      "je déclare reprise la session du parlement européen qui avait été interrompue le vendredi 00 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances .\n",
      "comme vous avez pu le constater , le grand \" bogue de l'an 0000 \" ne s'est pas produit . en revanche , les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles .\n"
     ]
    }
   ],
   "source": [
    "train_sentences = [line.strip() for line in open(\"data/french/train.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"data/french/valid.txt\").readlines()]\n",
    "test_sentences = [line.strip() for line in open(\"data/french/test.txt\").readlines()]\n",
    "\n",
    "train_sentences = [x for x in train_sentences if x] \n",
    "val_sentences = [x for x in val_sentences if x] \n",
    "test_sentences = [x for x in test_sentences if x] \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sentences[1])\n",
    "print(train_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#german pop\n",
    "\n",
    "\n",
    "sentences = val_sentences\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'le', 'quotidien', '>', 'a', '``', '>', '``', 'les', 'nouveaux', 'membres', 'du', 'conseil', 'municipal', 'appartenant', 'à', '>', ',', 'à', 'savoir', \"s'ils\", 'connaissent', '>', 'qui', 'est', 'né', 'au', 'cours', 'des', '>', 'années', 'à', 'la', '>', ',', 'quand', '>', 'les', 'partenaires', 'de', 'la', 'coalition', 'actuelle', 'qui', 'dirigeaient', 'à', 'prague', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "for ind,sen in enumerate(sentences):\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            sen.remove(\"<\")\n",
    "            sen.remove(\"unk\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#vocab french \\nvocabulary = [\"<SOS>\"] + [\"a\"] + [\"ä\"] + [\"á\"] + [\"à\"] + [\"â\"]+ [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"ê\"] + [\"é\"] + [\"è\"] + [\"ë\"] + [\"f\"] + [\"g\"] + [\"h\"] + [\"i\"] + [\"¡\"] + [\"ï\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"ñ\"] + [\"o\"] + [\"ó\"] + [\"ò\"] + [\"ô\"] + [\"í\"] + [\"î\"] + [\"ö\"] + [\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"ü\"] + [\"ú\"] + [\"ù\"] + [\"û\"] + [\"v\"] + [\"w\"] + [\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"\\'\"] + [\"`\"] + [\"``\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + [\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + [\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] + [\"ß\"] + [\"?\"] + [\"!\"] + [\"\\xad\"] + [\"ø\"] + [\"ç\"] + [\"+\"] + [\"æ\"] + [\"[\"] + [\"]\"] + [\"μ\"] + [\"å\"] + [\"\\'̧\"]\\n\\nmax_char_len = 524\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#vocab german\n",
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"ä\"] + [\"á\"] + [\"à\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"ê\"] + [\"é\"] + [\"è\"] + \\\n",
    "[\"ë\"] + [\"f\"] + [\"g\"] + [\"h\"] + [\"i\"] + [\"¡\"] + [\"ï\"] + [\"j\"] + [\"k\"] + [\"l\"] + \\\n",
    "[\"m\"] + [\"n\"] + [\"ñ\"] + [\"o\"] + [\"ó\"] + [\"ò\"] + [\"ô\"] + [\"í\"] + \\\n",
    "[\"ö\"] + [\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"ü\"] + [\"ú\"] + [\"û\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"`\"] + [\"``\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] + [\"ß\"] + [\"?\"] + [\"!\"] + \\\n",
    "[\"\\xad\"] + [\"ø\"] + [\"ç\"] + [\"+\"] + [\"æ\"] + [\"[\"] + [\"]\"] + [\"μ\"] + [\"å\"] + [\"'̧\"]\n",
    "\n",
    "max_char_len = 704\n",
    "'''\n",
    "\n",
    "'''\n",
    "#vocab spanish\n",
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"á\"] + [\"ä\"] + [\"à\"] + [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"é\"] + [\"ê\"] + \\\n",
    "[\"è\"] + [\"f\"] + [\"g\"] + [\"h\"] + [\"i\"] + [\"¡\"] + [\"ï\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"å\"] + \\\n",
    "[\"í\"] +[\"m\"] + [\"n\"] + [\"ñ\"] + [\"o\"] + \\\n",
    "[\"ó\"] + [\"ö\"] + [\"ô\"] + [\"ò\"] + [\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"ú\"] + [\"ü\"] + [\"û\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"`\"] + [\"``\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] + [\"¿\"] + [\"?\"] + [\"[\"] + \\\n",
    "[\"]\"] + [\"!\"] + [\"ø\"] + [\"ç\"] + [\"\\xad\"] + [\"+\"] + [\"μ\"] + [\"√\"] + [\"ß\"] + [\"æ\"]\n",
    "\n",
    "max_char_len = 700\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "#vocab russian\n",
    "vocabulary = [\"<SOS>\"] + [\"а\"] + [\"b\"] + [\"с\"] + [\"d\"] + [\"е\"] + [\"f\"] + [\"в\"] + [\"к\"] + [\"х\"] + [\"ё\"] + \\\n",
    "[\"g\"] + [\"h\"] + [\"i\"] + [\"j\"] + [\"k\"] + [\"l\"] + [\"m\"] + [\"n\"] + [\"о\"] + [\"н\"] + [\"я\"] + \\\n",
    "[\"р\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"v\"] + [\"w\"] + [\"з\"] + [\"г\"] + [\"т\"] + [\"м\"] + \\\n",
    "[\"x\"] + [\"у\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] + [\"и\"] + [\"ч\"] + [\"л\"] + [\"д\"] + \\\n",
    "[\"п\"] + [\"ц\"] + [\"ь\"] + [\"ы\"] + [\"б\"] + [\"щ\"] + [\"ш\"] + [\"э\"] + [\"ф\"] + [\"й\"] + [\"?\"] + [\"ж\"] + [\"ю\"] + [\"ъ\"] + [\"`\"] + \\\n",
    "[\"!\"] + [\"[\"] + [\"]\"] + [\"<\"] + [\"ц\"] + [\"+\"] + [\"=\"]\n",
    "\n",
    "max_char_len = 724\n",
    "'''\n",
    "\n",
    "'''\n",
    "#vocab french \n",
    "vocabulary = [\"<SOS>\"] + [\"a\"] + [\"ä\"] + [\"á\"] + [\"à\"] + [\"â\"]+ [\"b\"] + [\"c\"] + [\"d\"] + [\"e\"] + [\"ê\"] + [\"é\"] + [\"è\"] + \\\n",
    "[\"ë\"] + [\"f\"] + [\"g\"] + [\"h\"] + [\"i\"] + [\"¡\"] + [\"ï\"] + [\"j\"] + [\"k\"] + [\"l\"] + \\\n",
    "[\"m\"] + [\"n\"] + [\"ñ\"] + [\"o\"] + [\"ó\"] + [\"ò\"] + [\"ô\"] + [\"í\"] + [\"î\"] + \\\n",
    "[\"ö\"] + [\"p\"] + [\"q\"] + [\"r\"] + [\"s\"] + [\"t\"] + [\"u\"] + [\"ü\"] + [\"ú\"] + [\"ù\"] + [\"û\"] + [\"v\"] + [\"w\"] + \\\n",
    "[\"x\"] + [\"y\"] + [\"z\"] + [\"<EOW>\"] + [\"<EOS>\"] + [\">\"] + [\"-\"] + [\".\"] + [\"'\"] + [\"`\"] + [\"``\"] + [\"0\"] + [\"1\"] + [\"2\"] + [\"3\"] + \\\n",
    "[\"4\"] + [\"5\"] + [\"6\"] + [\"7\"] + [\"8\"] + [\"9\"] + [\"&\"] + [\"<\"] + [\"$\"] + [\"#\"] + [\"/\"] + [\",\"] + [\"|\"] + \\\n",
    "[\"@\"] + [\"%\"] + [\"^\"] + [\"\\\\\"] + [\"*\"] + [\"(\"] + [\")\"] + [\"{\"] + [\"}\"] + [\":\"] + [\";\"] + [\"ß\"] + [\"?\"] + [\"!\"] + \\\n",
    "[\"\\xad\"] + [\"ø\"] + [\"ç\"] + [\"+\"] + [\"æ\"] + [\"[\"] + [\"]\"] + [\"μ\"] + [\"å\"] + [\"'̧\"]\n",
    "\n",
    "max_char_len = 524\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "а\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "token2index = {token:index for index,token in enumerate(vocabulary)}\n",
    "index2token = {index:token for index,token in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabulary_size)\n",
    "print(token2index.get(\"*\"))\n",
    "print(index2token.get(1))\n",
    "print(one_hot_embeddings[token2index.get(\"\\\\\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "2314\n"
     ]
    }
   ],
   "source": [
    "max_word_length = 0\n",
    "maxid = 0\n",
    "for i in range(len(sentences)):\n",
    "    l = len(sentences[i])\n",
    "    if l > max_word_length:\n",
    "        maxid = i\n",
    "        max_word_length = l\n",
    "        \n",
    "print(max_word_length) \n",
    "print(maxid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_producer(sentences):\n",
    "    s_tensor = np.empty((len(sentences),max_char_len,vocabulary_size))\n",
    "    word_loc_all = np.zeros((len(sentences),max_word_length))\n",
    "    eow_loc_all = np.zeros((len(sentences),max_char_len))\n",
    "    sen_lens = []\n",
    "    num_words = []\n",
    "    max_char = 0\n",
    "    #for s in sentences[23478:23480]:\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        embed = np.zeros((max_char_len,vocabulary_size))\n",
    "        word_loc = np.zeros(max_word_length)\n",
    "        eow_loc = np.zeros(max_char_len)\n",
    "        prev = 0\n",
    "        count = 0 \n",
    "        #print(i)\n",
    "        for k in range(len(s)):\n",
    "            w = s[k]\n",
    "            #print(w)\n",
    "            for id,token in enumerate(w):\n",
    "                \n",
    "                if (w == \"<EOS>\") | (w == \"<SOS>\") | (w == \">\"):\n",
    "                    break\n",
    "                else:\n",
    "                    #print(prev + id)\n",
    "                    #print(token)\n",
    "                    count+=1\n",
    "                    embed[prev + id,:] = np.squeeze(one_hot_embeddings[token2index.get(token)])\n",
    "                \n",
    "            if (w == \"<EOS>\") | (w == \"<SOS>\"):\n",
    "                word_loc[k] = id + 1\n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + id + 1 \n",
    "                \n",
    "            elif (w == \">\"):\n",
    "                word_loc[k] = id + 1\n",
    "                count +=1\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(w)]\n",
    "                prev = prev + id + 1 \n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1\n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            else: \n",
    "                prev = prev + id + 1\n",
    "                word_loc[k] = id + 1 \n",
    "                #print(prev)\n",
    "                embed[prev,:] = one_hot_embeddings[token2index.get(\"<EOW>\")]\n",
    "                count +=1 \n",
    "                eow_loc[count] = 1\n",
    "                prev = prev + 1\n",
    "                \n",
    "            \n",
    "        s_tensor[i,:,:] = embed\n",
    "        eow_loc_all[i,:] = eow_loc\n",
    "        n_w = int(np.sum(eow_loc_all[i]))\n",
    "        \n",
    "        num_words.append(2*n_w - 1)\n",
    "        sen_lens.append(count+1)\n",
    "        \n",
    "        if (count+1 > max_char):\n",
    "            max_char = count+1 \n",
    "            \n",
    "        #to get word end locations to retrieve hidden states later \n",
    "        word_loc_all[i,0] = word_loc[0]\n",
    "        for j in range(1,len(s)):\n",
    "            word_loc_all[i,j] = word_loc_all[i,j-1] + word_loc[j]\n",
    "            \n",
    "        \n",
    "    return s_tensor,word_loc_all,eow_loc_all,max_char \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626\n"
     ]
    }
   ],
   "source": [
    "data,word_loc_all,eow_loc_all,max_char = embed_producer(sentences)\n",
    "\n",
    "print(max_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[4607])\n",
    "print(word_loc_all[4607])\n",
    "print(eow_loc_all[4607])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
